<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>07-reward-models</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/usr/share/javascript/mathjax/MathJax.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#reward-modeling" id="toc-reward-modeling"><span
class="toc-section-number">1</span> Reward Modeling</a>
<ul>
<li><a href="#training-reward-models"
id="toc-training-reward-models"><span
class="toc-section-number">1.1</span> Training Reward Models</a></li>
<li><a href="#architecture" id="toc-architecture"><span
class="toc-section-number">1.2</span> Architecture</a></li>
<li><a href="#implementation-example"
id="toc-implementation-example"><span
class="toc-section-number">1.3</span> Implementation Example</a></li>
<li><a href="#variants" id="toc-variants"><span
class="toc-section-number">1.4</span> Variants</a>
<ul>
<li><a href="#preference-margin-loss"
id="toc-preference-margin-loss"><span
class="toc-section-number">1.4.1</span> Preference Margin Loss</a></li>
<li><a href="#balancing-multiple-comparisons-per-prompt"
id="toc-balancing-multiple-comparisons-per-prompt"><span
class="toc-section-number">1.4.2</span> Balancing Multiple Comparisons
Per Prompt</a></li>
<li><a href="#k-wise-loss-function" id="toc-k-wise-loss-function"><span
class="toc-section-number">1.4.3</span> K-wise Loss Function</a></li>
</ul></li>
<li><a href="#outcome-reward-models"
id="toc-outcome-reward-models"><span
class="toc-section-number">1.5</span> Outcome Reward Models</a></li>
<li><a href="#process-reward-models"
id="toc-process-reward-models"><span
class="toc-section-number">1.6</span> Process Reward Models</a></li>
<li><a
href="#reward-models-vs.-outcome-rms-vs.-process-rms-vs.-value-functions"
id="toc-reward-models-vs.-outcome-rms-vs.-process-rms-vs.-value-functions"><span
class="toc-section-number">1.7</span> Reward Models vs. Outcome RMs
vs. Process RMs vs. Value Functions</a></li>
<li><a href="#generative-reward-modeling"
id="toc-generative-reward-modeling"><span
class="toc-section-number">1.8</span> Generative Reward
Modeling</a></li>
<li><a href="#further-reading" id="toc-further-reading"><span
class="toc-section-number">1.9</span> Further Reading</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="reward-modeling"><span
class="header-section-number">1</span> Reward Modeling</h1>
<p>Reward models are core to the modern approach to RLHF. Reward models
broadly have been used extensively in reinforcement learning research as
a proxy for environment rewards <span class="citation"
data-cites="sutton2018reinforcement">[@sutton2018reinforcement]</span>.
The practice is closely related to inverse reinforcement learning, where
the problem is to approximate an agent’s reward function given
trajectories of behavior <span class="citation"
data-cites="ng2000algorithms">[@ng2000algorithms]</span>, and other
areas of deep reinforcement learning. Reward models were proposed, in
their modern form, as a tool for studying the value alignment problem
<span class="citation"
data-cites="leike2018scalable">[@leike2018scalable]</span>.</p>
<p>The most common reward model predicts the probability that a piece of
text was close to a “preferred” piece of text from the training
comparisons. Later in this section we also compare these to Outcome
Reward Models (ORMs) that predict the probability and a completion
results in a correct answer or a Process Reward Model (PRM) that assigns
a score to each step in reasoning. When not indicated, the reward models
mentioned are those predicting preference between text.</p>
<h2 data-number="1.1" id="training-reward-models"><span
class="header-section-number">1.1</span> Training Reward Models</h2>
<p>There are two popular expressions for how to train a standard reward
model for RLHF – they are numerically equivalent. The canonical
implementation is derived from the Bradley-Terry model of preference
<span class="citation" data-cites="BradleyTerry">[@BradleyTerry]</span>.
A Bradley-Terry model of preferences measures the probability that the
pairwise comparison for two events drawn from the same distribution, say
<span class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span>, satisfy the following relation, <span
class="math inline">\(i &gt; j\)</span>:</p>
<p><span class="math display">\[P(i &gt; j) = \frac{p_i}{p_i +
p_j}\]</span> {#eq:bradterry}</p>
<p>To train a reward model, we must formulate a loss function that
satisfies the above relation. The first structure applied is to convert
a language model into a model that outputs a scalar value, often in the
form of a single classification probability logit. Thus, we can take the
score of this model with two samples, the <span
class="math inline">\(i\)</span> and <span
class="math inline">\(j\)</span> above are now completions, <span
class="math inline">\(y_1\)</span> and <span
class="math inline">\(y_2\)</span>, to one prompt, <span
class="math inline">\(x\)</span> and score both of them with respect to
the above model, <span class="math inline">\(r_\theta\)</span>.</p>
<p>The probability of success for a given reward model in a pairwise
comparison, becomes:</p>
<p><span class="math display">\[P(y_1 &gt; y_2) =
\frac{\exp(r(y_1))}{\exp(r(y_1)) + \exp(r(y_2))}\]</span>
{#eq:bradterryrm}</p>
<p>Then, by taking the gradient with respect to the model parameters, we
can arrive at the loss function to train a reward model. The first form,
as in <span class="citation"
data-cites="ouyang2022training">[@ouyang2022training]</span> and other
works: <span class="math display">\[\mathcal{L}(\theta) = - \log \left(
\sigma \left( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \right)
\right)\]</span> {#eq:rewardmodeling1}</p>
<p>Second, as in <span class="citation"
data-cites="askell2021general">[@askell2021general]</span> and other
works: <span class="math display">\[\mathcal{L}(\theta) = \log \left( 1
+ e^{r_{\theta}(x, y_l) - r_{\theta}(x, y_w)} \right)\]</span>
{#eq:rewardmodeling2}</p>
<h2 data-number="1.2" id="architecture"><span
class="header-section-number">1.2</span> Architecture</h2>
<p>The most common way reward models are implemented is through an
abstraction similar to Transformer’s
<code>AutoModelForSequenceClassification</code>, which appends a small
linear head to the language model that performs classification between
two outcomes – chosen and rejected. At inference time, the model outputs
the <em>probability that the piece of text is chosen</em> as a single
logit from the model.</p>
<p>Other implementation options exist, such as just taking a linear
layer directly from the final embeddings, but they are less common in
open tooling.</p>
<h2 data-number="1.3" id="implementation-example"><span
class="header-section-number">1.3</span> Implementation Example</h2>
<p>Implementing the reward modeling loss is quite simple. More of the
implementation challenge is on setting up a separate data loader and
inference pipeline. Given the correct dataloader, the loss is
implemented as:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>rewards_chosen <span class="op">=</span> model(<span class="op">**</span>inputs_chosen)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>rewards_rejected <span class="op">=</span> model(<span class="op">**</span>inputs_rejected)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span>nn.functional.logsigmoid(rewards_chosen <span class="op">-</span> rewards_rejected).mean()</span></code></pre></div>
<p>Note, when training reward models, the most common practice is to
train for only 1 epoch to avoid overfitting.</p>
<h2 data-number="1.4" id="variants"><span
class="header-section-number">1.4</span> Variants</h2>
<p>Reward modeling is a relatively under-explored area of RLHF. The
traditional reward modeling loss has been modified in many popular
works, but the modifications have not solidified into a single best
practice.</p>
<h3 data-number="1.4.1" id="preference-margin-loss"><span
class="header-section-number">1.4.1</span> Preference Margin Loss</h3>
<p>In the case where annotators are providing either scores or rankings
on a Likert Scale, the magnitude of the relational quantities can be
used in training. The most common practice is to binarize the data
direction, implicitly scores of 1 and 0, but the additional information
has been used to improve model training. Llama 2 proposes using the
margin between two datapoints, <span
class="math inline">\(m(r)\)</span>, to distinguish the magnitude of
preference:</p>
<p><span class="math display">\[\mathcal{L}(\theta) = - \log \left(
\sigma \left( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) - m(r) \right)
\right)\]</span> {#eq:rewardmodelingmargin}</p>
<p>Note that in Llama 3 the margin term was removed as the team observed
diminishing improvements after scaling.</p>
<h3 data-number="1.4.2"
id="balancing-multiple-comparisons-per-prompt"><span
class="header-section-number">1.4.2</span> Balancing Multiple
Comparisons Per Prompt</h3>
<p>InstructGPT studies the impact of using a variable number of
completions per prompt, yet balancing them in the reward model training
<span class="citation"
data-cites="ouyang2022training">[@ouyang2022training]</span>. To do
this, they weight the loss updates per comparison per prompt. At an
implementation level, this can be done automatically by including all
examples with the same prompt in the same training batch, naturally
weighing the different pairs – not doing this caused overfitting to the
prompts. The loss function becomes:</p>
<p><span class="math display">\[\mathcal{L}(\theta) = -
\frac{1}{\binom{K}{2}} \mathbb{E}_{(x, y_w, y_l)\sim D} \log \left(
\sigma \left( r_{\theta}(x, y_w) - r_{\theta}(x, y_l) \right)
\right)\]</span> {#eq:rewardmodelinginstructgpt}</p>
<h3 data-number="1.4.3" id="k-wise-loss-function"><span
class="header-section-number">1.4.3</span> K-wise Loss Function</h3>
<p>There are many other formulations that can create suitable models of
human preferences for RLHF. One such example, used in the popular, early
RLHF’d models Starling 7B and 34B <span class="citation"
data-cites="zhu2024starling">[@zhu2024starling]</span>, is a K-wise loss
function based on the Plackett-Luce model <span class="citation"
data-cites="liu2019learning">[@liu2019learning]</span>.</p>
<p>Following Zhu et al. 2023 formalizes the setup <span class="citation"
data-cites="zhu2023principled">[@zhu2023principled]</span>, following as
follows. With a prompt, or state, <span
class="math inline">\(s^i\)</span>, <span
class="math inline">\(K\)</span> actions <span
class="math inline">\((a_0^i, a_1^i, \cdots, a_{K-1}^i)\)</span> are
sampled from <span
class="math inline">\(P(a_0,\cdots,a_{K-1}|s^i)\)</span>. Then, labelers
are used to rank preferences with <span class="math inline">\(\sigma^i:
[K] \mapsto [K]\)</span> is a function representing action rankings,
where <span class="math inline">\(\sigma^i(0)\)</span> is the most
preferred action. This yields a preference model capturing the
following:</p>
<p><span
class="math display">\[P(\sigma^i|s^i,a_0^i,a_1^i,\ldots,a_{K-1}^i) =
\prod_{k=0}^{K-1}
\frac{\exp(r_{\theta\star}(s^i,a_{\sigma^i(k)}^i))}{\sum_{j=k}^{K-1}\exp(r_{\theta\star}(s^i,a_{\sigma^i(j)}^i))}\]</span>
{#eq:kwise_rm}</p>
<p>When <span class="math inline">\(K = 2\)</span>, this reduces to the
Bradley-Terry (BT) model for pairwise comparisons. Regardless, once
trained, these models are used similarly to other reward models during
RLHF training.</p>
<h2 data-number="1.5" id="outcome-reward-models"><span
class="header-section-number">1.5</span> Outcome Reward Models</h2>
<p>The majority of <em>preference tuning</em> for language models and
other AI systems is done with the Bradley Terry models discussed above.
For reasoning heavy tasks, one can use an Outcome Reward Model (ORM).
The training data for an ORM is constructed in a similar manner to
standard preference tuning. Here, we have a problem statement or prompt,
<span class="math inline">\(x\)</span> and two completions <span
class="math inline">\(y_1\)</span> and <span
class="math inline">\(y_2\)</span>. The inductive bias used here is that
one completion should be a correct solution to the problem and one
incorrect, resulting in <span
class="math inline">\((y_c,y_{ic})\)</span>.</p>
<p>The shape of the models used is very similar to a standard reward
model, with a linear layer appended to a model that can output a single
logit (in the case of an RM) – with an ORM, the training objective that
follows is slightly different <span class="citation"
data-cites="cobbe2021training">[@cobbe2021training]</span>:</p>
<blockquote>
<p>[We] train verifiers with a joint objective where the model learns to
label a model completion as correct or incorrect, in addition to the
original language modeling objective. Architecturally, this means our
verifiers are language models, with a small scalar head that outputs
predictions on a per-token basis. We implement this scalar head as a
single bias parameter and single gain parameter that operate on the
logits outputted by the language model’s final unembedding layer.</p>
</blockquote>
<p>To translate, this is implemented as a language modeling head that
can predict two classes per token (1 for correct, 0 for incorrect),
rather than a classification head of a traditional RM that outputs one
token for the entire sequence. Formally, following <span
class="citation"
data-cites="lyu2025exploring">[@lyu2025exploring]</span> this can be
shown as:</p>
<p><span class="math display">\[\mathcal{L}_{\text{CE}} =
-\mathbb{E}_{(s,r)\sim \mathcal{D}}[r\log p_\theta(s) +
(1-r)\log(1-p_\theta(s))]\]</span> {#eq:orm_loss}</p>
<p>where <span class="math inline">\(r \in {0,1}\)</span> is a binary
label where 1 applies to a correct answer to a given prompt and 0
applies to an incorrect, and <span
class="math inline">\(p_\theta(s)\)</span> is the scalar proportional to
predicted probability of correctness from the model being trained.</p>
<p>These models have continued in use, but are less supported in
open-source RLHF tools. For example, the same type of ORM was used in
the seminal work <em>Let’s Verify Step by Step</em> <span
class="citation" data-cites="lightman2023let">[@lightman2023let]</span>,
but without the language modeling prediction piece of the loss. Then,
the final loss is a cross entropy loss on every token predicting if the
final answer is correct.</p>
<p>Given the lack of support, the term outcome reward model (ORM) has
been used in multiple ways. Some literature, e.g. <span class="citation"
data-cites="lyu2025exploring">[@lyu2025exploring]</span>, continues to
use the original definition from Cobbe et al. 2021. Others do not.</p>
<h2 data-number="1.6" id="process-reward-models"><span
class="header-section-number">1.6</span> Process Reward Models</h2>
<p>Process Reward Models (PRMs), originally called Process-supervised
Reward Models, are reward models trained to output scores at every
<em>step</em> in a chain of thought reasoning process. These differ from
a standard RM that outputs a score only at an EOS token or a ORM that
outputs a score at every token. Process Reward Models require
supervision at the end of each reasoning step, and then are trained
similarly where the tokens in the step are trained to their relevant
target – the target is the step in PRMs and the entire response for
ORMs.</p>
<p>Here’s an example of how this per-step label can be packaged in a
trainer, from HuggingFace’s TRL <span class="citation"
data-cites="vonwerra2022trl">[@vonwerra2022trl]</span>:</p>
<pre><code># Get the ID of the separator token and add it to the completions
separator_ids = tokenizer.encode(step_separator, add_special_tokens=False)
completions_ids = [completion + separator_ids for completion in completions_ids]

# Create the label 
labels = [[-100] * (len(completion) - 1) + [label] for completion, label in zip(completions_ids, labels)]</code></pre>
<p>Traditionally PRMs are trained with a language modeling head that
outputs a token only at the end of a reasoning step, e.g. at the token
corresponding to a double new line or other special token. These
predictions tend to be -1 for incorrect, 0 for neutral, and 1 for
correct. These labels do not necessarily tie with whether or not the
model is on the right path, but if the step is correct.</p>
<h2 data-number="1.7"
id="reward-models-vs.-outcome-rms-vs.-process-rms-vs.-value-functions"><span
class="header-section-number">1.7</span> Reward Models vs. Outcome RMs
vs. Process RMs vs. Value Functions</h2>
<p>The various types of reward models covered indicate the spectrum of
ways that “quality” can be measured in RLHF and other post-training
methods. Below, a summary of what the models predict and how they are
trained.</p>
<table>
<caption>Comparing types of reward models. {#tbl:rm_compare}</caption>
<colgroup>
<col style="width: 18%" />
<col style="width: 27%" />
<col style="width: 32%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="header">
<th>Model Class</th>
<th>What They Predict</th>
<th>How They Are Trained</th>
<th>LM structure</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Reward Models</strong></td>
<td>Quality of text via probability of chosen response at EOS token</td>
<td>Contrastive loss between pairwise (or N-wise) comparisons between
completions</td>
<td>Regression or classification head on top of LM features</td>
</tr>
<tr class="even">
<td><strong>Outcome Reward Models</strong></td>
<td>Probability that an answer is correct per-token</td>
<td>Labeled outcome pairs (e.g., success/failure on verifiable
domains)</td>
<td>Language modeling head per-token cross-entropy, where every label is
the outcome level label</td>
</tr>
<tr class="odd">
<td><strong>Process Reward Models</strong></td>
<td>A reward or score for intermediate steps at end of reasoning
steps</td>
<td>Trained using intermediate feedback or stepwise annotations (trained
per token in reasoning step)</td>
<td>Language modeling head only running inference per reasoning step,
predicts three classes -1, 0, 1</td>
</tr>
<tr class="even">
<td><strong>Value Functions</strong></td>
<td>The expected return given the current state</td>
<td>Trained via regression to each point in sequence</td>
<td>A classification with output per-token</td>
</tr>
</tbody>
</table>
<p>Some notes, given the above table has a lot of edge cases.</p>
<ul>
<li>Both in preference tuning and reasoning training, the value
functions often have a discount factor of 1, which makes a value
function even closer to an outcome reward model, but with a different
training loss.</li>
<li>A process reward model can be supervised by doing rollouts from an
intermediate state and collecting outcome data. This blends multiple
ideas, but if the <em>loss</em> is per reasoning step labels, it is best
referred to as a PRM.</li>
</ul>
<h2 data-number="1.8" id="generative-reward-modeling"><span
class="header-section-number">1.8</span> Generative Reward Modeling</h2>
<p>With the cost of preference data, a large research area emerged to
use existing language models as a judge of human preferences or in other
evaluation settings <span class="citation"
data-cites="zheng2023judging">[@zheng2023judging]</span>. The core idea
is to prompt a language model with instructions on how to judge, a
prompt, and two completions (much as would be done with human labelers).
An example prompt, from one of the seminal works here for the chat
evaluation MT-Bench <span class="citation"
data-cites="zheng2023judging">[@zheng2023judging]</span>, follows:</p>
<pre><code>[System]
Please act as an impartial judge and evaluate the quality of the responses provided by two
AI assistants to the user question displayed below. You should choose the assistant that
follows the user’s instructions and answers the user’s question better. Your evaluation
should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
and level of detail of their responses. Begin your evaluation by comparing the two
responses and provide a short explanation. Avoid any position biases and ensure that the
order in which the responses were presented does not influence your decision. Do not allow
the length of the responses to influence your evaluation. Do not favor certain names of
the assistants. Be as objective as possible. After providing your explanation, output your
final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot;
if assistant B is better, and &quot;[[C]]&quot; for a tie.
[User Question]
{question}
[The Start of Assistant A’s Answer]
{answer_a}
[The End of Assistant A’s Answer]
[The Start of Assistant B’s Answer]
{answer_b}
[The End of Assistant B’s Answer]</code></pre>
<p>Given the efficacy of LLM-as-a-judge for evaluation, spawning many
other evaluations such as AlpacaEval <span class="citation"
data-cites="dubois2024length">[@dubois2024length]</span>, Arena-Hard
<span class="citation"
data-cites="li2024crowdsourced">[@li2024crowdsourced]</span>, and
WildBench <span class="citation"
data-cites="lin2024wildbench">[@lin2024wildbench]</span>, many began
using LLM-as-a-judge instead of reward models to create and use
preference data.</p>
<p>An entire field of study has emerged to study how to use so called
“Generative Reward Models” <span class="citation"
data-cites="mahan2024generative">[@mahan2024generative]</span> <span
class="citation"
data-cites="zhang2024generative">[@zhang2024generative]</span> <span
class="citation"
data-cites="ankner2024critique">[@ankner2024critique]</span> (including
models trained <em>specifically</em> to be effective judges <span
class="citation"
data-cites="kim2023prometheus">[@kim2023prometheus]</span>), but on RM
evaluations they tend to be behind existing reward models, showing that
reward modeling is an important technique for current RLHF.</p>
<p>A common trick to improve the robustness of LLM-as-a-judge workflows
is to use a sampling temperature of 0 to reduce variance of ratings.</p>
<h2 data-number="1.9" id="further-reading"><span
class="header-section-number">1.9</span> Further Reading</h2>
<p>The academic literature for reward modeling established itself in
2024. The bulk of progress in reward modeling early on has been in
establishing benchmarks and identifying behavior modes. The first RM
benchmark, RewardBench, provided common infrastructure for testing
reward models <span class="citation"
data-cites="lambert2024rewardbench">[@lambert2024rewardbench]</span>.
Since then, RM evaluation has expanded to be similar to the types of
evaluations available to general post-trained models, where some
evaluations test the accuracy of prediction on domains with known true
answers <span class="citation"
data-cites="lambert2024rewardbench">[@lambert2024rewardbench]</span> or
those more similar to “vibes” performed with LLM-as-a-judge or
correlations to other benchmarks <span class="citation"
data-cites="wen2024rethinking">[@wen2024rethinking]</span>.</p>
<p>Examples of new benchmarks include multilingual reward bench
(M-RewardBench) <span class="citation"
data-cites="gureja2024m">[@gureja2024m]</span>, RAG-RewardBench <span
class="citation" data-cites="jin2024rag">[@jin2024rag]</span>, RMB <span
class="citation" data-cites="zhou2024rmb">[@zhou2024rmb]</span> or
RM-Bench <span class="citation"
data-cites="liu2024rm">[@liu2024rm]</span> for general chat,ReWordBench
for typos <span class="citation"
data-cites="wu2025rewordbench">[@wu2025rewordbench]</span>, MJ-Bench
<span class="citation" data-cites="chen2024mj">[@chen2024mj]</span>,
Multimodal RewardBench <span class="citation"
data-cites="yasunaga2025multimodal">[@yasunaga2025multimodal]</span>, VL
RewardBench <span class="citation"
data-cites="li2024vlrewardbench">[@li2024vlrewardbench]</span>, or
VLRMBench <span class="citation"
data-cites="ruan2025vlrmbench">[@ruan2025vlrmbench]</span> for vision
language models, Preference Proxy Evaluations <span class="citation"
data-cites="frick2024evaluate">[@frick2024evaluate]</span>, and
RewardMATH <span class="citation"
data-cites="kim2024evaluating">[@kim2024evaluating]</span>. Process
reward models (PRMs) have their own emerging benchmarks, such as PRM
Bench <span class="citation"
data-cites="song2025prmbench">[@song2025prmbench]</span> and visual
benchmarks of VisualProcessBench <span class="citation"
data-cites="wang2025visualprm">[@wang2025visualprm]</span> and ViLBench
<span class="citation"
data-cites="tu2025vilbench">[@tu2025vilbench]</span>.</p>
<p>To understand progress on <em>training</em> reward models, one can
reference new reward model training methods, with aspect-conditioned
models <span class="citation"
data-cites="wang2024interpretable">[@wang2024interpretable]</span>, high
quality human datasets <span class="citation"
data-cites="wang2024helpsteer2">[@wang2024helpsteer2]</span> <span
class="citation"
data-cites="wang2024helpsteer2p">[@wang2024helpsteer2p]</span>, scaling
<span class="citation"
data-cites="adler2024nemotron">[@adler2024nemotron]</span>, extensive
experimentation <span class="citation"
data-cites="touvron2023llama">[@touvron2023llama]</span>, or debiasing
data <span class="citation"
data-cites="park2024offsetbias">[@park2024offsetbias]</span>.</p>
</body>
</html>
