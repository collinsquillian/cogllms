<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>09-instruction-tuning</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#instruction-finetuning"
id="toc-instruction-finetuning"><span
class="toc-section-number">1</span> Instruction Finetuning</a>
<ul>
<li><a href="#chat-templates-and-the-structure-of-instructions"
id="toc-chat-templates-and-the-structure-of-instructions"><span
class="toc-section-number">1.1</span> Chat templates and the structure
of instructions</a></li>
<li><a href="#best-practices-of-instruction-tuning"
id="toc-best-practices-of-instruction-tuning"><span
class="toc-section-number">1.2</span> Best practices of instruction
tuning</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="instruction-finetuning"><span
class="header-section-number">1</span> Instruction Finetuning</h1>
<p>Early language models were only trained to predict the next tokens in
a sequence and were not adapted to any specific tasks. Around the
release of GPT-3 <span class="citation"
data-cites="brown2020language">[@brown2020language]</span>, language
models were still primarily used via in-context learning where examples
were shown to the model and then it was asked to complete a similar
task.</p>
<p>This was the combination of two trends – historically in the natural
language processing (NLP) literature, models were trained for a specific
task. Here, as seen with one example where bigger models generalize
better, multiple results showed how standardizing the approach of task
data can enable dramatically different downstream performance. Prominent
examples of unifying the framework for tasks includes <em>Exploring the
Limits of Transfer Learning with a Unified Text-to-Text Transformer</em>
(T5 models) <span class="citation"
data-cites="raffel2020exploring">[@raffel2020exploring]</span>,
<em>Finetuned Language Models Are Zero-Shot Learners</em> (FLAN
dataset)<span class="citation"
data-cites="wei2021finetuned">[@wei2021finetuned]</span>, <em>Multitask
Prompted Training Enables Zero-Shot Task Generalization</em> (T0 models)
<span class="citation"
data-cites="sanh2021multitask">[@sanh2021multitask]</span>, and
<em>Cross-Task Generalization via Natural Language Crowdsourcing
Instructions</em> (Natural Instructions dataset) <span class="citation"
data-cites="mishra2021cross">[@mishra2021cross]</span>. These insights
led to the era of <em>finetuning</em> language models. Historically,
until RLHF and related methods, all finetuning was <strong>instruction
finetuning</strong> (IFT), also known as <strong>supervised
finetuning</strong>.</p>
<p>Since, instruction finetuning, also called colloquially just
<em>instruction tuning</em>, has matured and is standard practice across
many language modeling pipelines. At its core, IFT is the simplest
method for adapting language models to a desired task. It serves as the
foundation for RLHF by preparing the model for a format of instructions
that is known common, question-answering, and is the first tool used by
those attempting to apply modern techniques to new domains.</p>
<p>Instruction tuning practically uses the same autoregressive loss
function used in pretraining language models.</p>
<h2 data-number="1.1"
id="chat-templates-and-the-structure-of-instructions"><span
class="header-section-number">1.1</span> Chat templates and the
structure of instructions</h2>
<p>A core piece of the RLHF process is making it so user queries are
formatted in a format that is easily readable by a tokenizer and the
associated language model. The tool that handles the structure of the
interaction with the user is called the <strong>chat
template</strong>.</p>
<p>An example which we will break down is below:</p>
<pre class="jinja"><code>{% if messages[0][&#39;role&#39;] == &#39;system&#39; %}
    {% set offset = 1 %}
{% else %}
    {% set offset = 0 %}
{% endif %}

{{ bos_token }}
{% for message in messages %}
    {% if (message[&#39;role&#39;] == &#39;user&#39;) != (loop.index0 % 2 == offset) %}
        {{ raise_exception(&#39;Conversation roles must alternate user/assistant/user/assistant/...&#39;) }}
    {% endif %}

    {{ &#39;&lt;|im_start|&gt;&#39; + message[&#39;role&#39;] + &#39;\n&#39; + message[&#39;content&#39;] | trim + &#39;&lt;|im_end|&gt;\n&#39; }}
{% endfor %}

{% if add_generation_prompt %}
    {{ &#39;&lt;|im_start|&gt;assistant\n&#39; }}
{% endif %}</code></pre>
<p>This is the raw code for transforming a list of dictionaries in
Python containing messages and roles into tokens that a language model
can predict from.</p>
<p>All information passed into models is assigned a role. The
traditional three roles are <code>system</code>, <code>user</code>, and
<code>assistant</code>.</p>
<p>The <code>system</code> tag is only used for the first message of the
conversation which hold instructions for the agent in text that will not
be received from or exposed to the user. These <strong>system
prompts</strong> are used to provide additional context to the models,
such as the date and time, or to patch behaviors. As a fun example,
models can be told things such as “You are a friendly chatbot who always
responds in the style of a pirate.”</p>
<p>Next, the two other roles are logical, as <strong>user</strong> is
the messages from the one using the AI, and <strong>assistant</strong>
holds the responses from the user.</p>
<p>In order to translate all this information into tokens, we use the
code listing above that we started with. The model has a series of
<em>special tokens</em> that separate the various messages from each
other. If we run the above code with the example query “How many
helicopters can a human eat in one sitting?” the next passed into the
model would look as follows:</p>
<pre><code>&lt;|system|&gt;
You are a friendly chatbot who always responds in the style of a pirate&lt;/s&gt;
&lt;|user|&gt;
How many helicopters can a human eat in one sitting?&lt;/s&gt;
&lt;|assistant|&gt;</code></pre>
<p>Notices how the final token in the sequence is
<code>&lt;|assistant|&gt;</code>, this is how the model knows to
continue generating tokens until it finally generates its end of
sequence token, which in this case is <code>&lt;/s&gt;</code>.</p>
<p>By packing all question-answer pair data (and downstream preference
tuning data) into this format, modern language models follow it with
perfect consistency. This is the language that instruction tuned models
use to exchange information with users and the models stored on GPUs or
other computing devices.</p>
<p>The behavior can be extended naively to multiple turns, such as shown
below:</p>
<pre><code>&lt;|system|&gt;
You are a friendly chatbot who always responds in the style of a pirate&lt;/s&gt;
&lt;|user|&gt;
How many helicopters can a human eat in one sitting?&lt;/s&gt;
&lt;|assistant|&gt;
Oh just 6.&lt;/s&gt;
&lt;|user|&gt;
Are you sure about that?&lt;/s&gt;
&lt;|assistant|&gt;</code></pre>
<p>In the open ecosystem, the standard method for applying the chat
template to a list of messages is a piece of jinja code saved in the
tokenizer, as <code>apply_chat_template</code>.</p>
<h2 data-number="1.2" id="best-practices-of-instruction-tuning"><span
class="header-section-number">1.2</span> Best practices of instruction
tuning</h2>
<p>Instruction tuning as the foundation of post-training and creating
helpful language models is well-established. There are many ways to
achieve successful instruction tuning. For example, efficient finetuning
with quantization of some model parameters makes training very
accessible <span class="citation"
data-cites="dettmers2023qlora">[@dettmers2023qlora]</span>. Also, in
narrow domains such as chat alignment, i.e. without harder skills such
as math or code, small, focused datasets can achieve strong performance
<span class="citation"
data-cites="zhou2023lima">[@zhou2023lima]</span>.</p>
<p>Soon after the release of ChatGPT, human datasets with as few as 10K
samples such as No Robots were state-of-the-art <span class="citation"
data-cites="no_robots">[@no_robots]</span>. Years later, large-scale
synthetic datasets work best <span class="citation"
data-cites="lambert2024t">[@lambert2024t]</span> on most tasks.</p>
<p>A few principles remain:</p>
<ul>
<li>High-quality data is key to performance. The completions are what
the model actually learns from (in many cases the prompts are not
predicted over so the model does not learn to predict prompts).</li>
<li>~1M prompts can be used to create a model capable of excellent RLHF
and post-training. Further scaling prompts can have improvements, but
has quick diminishing returns.</li>
<li>The best prompts are those in a similar distribution to downstream
tasks of interest.</li>
<li>If multiple stages of training are done after instruction tuning,
the models can recover from some noise in the process. Optimizing the
overall optimization is more important than each individual stage.</li>
</ul>
</body>
</html>
