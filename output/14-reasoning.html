<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>14-reasoning</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#reasoning-training-inference-time-scaling"
id="toc-reasoning-training-inference-time-scaling"><span
class="toc-section-number">1</span> Reasoning Training &amp;
Inference-Time Scaling</a>
<ul>
<li><a href="#why-does-rl-work-now" id="toc-why-does-rl-work-now"><span
class="toc-section-number">1.1</span> Why Does RL Work Now?</a></li>
<li><a href="#rl-training-vs.-inference-time-scaling"
id="toc-rl-training-vs.-inference-time-scaling"><span
class="toc-section-number">1.2</span> RL Training vs. Inference Time
Scaling</a></li>
<li><a href="#the-future-beyond-reasoning-of-reinforcement-finetuning"
id="toc-the-future-beyond-reasoning-of-reinforcement-finetuning"><span
class="toc-section-number">1.3</span> The Future (Beyond Reasoning) of
Reinforcement Finetuning</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="reasoning-training-inference-time-scaling"><span
class="header-section-number">1</span> Reasoning Training &amp;
Inference-Time Scaling</h1>
<p>At the 2016 edition of the Neural Information Processing Systems
(NeurIPS) conference, Yann LeCun first introduced his now-famous cake
metaphor for where learning happens in modern machine learning
systems:</p>
<blockquote>
<p>If intelligence is a cake, the bulk of the cake is unsupervised
learning, the icing on the cake is supervised learning, and the cherry
on the cake is reinforcement learning (RL).</p>
</blockquote>
<p>This analogy is now largely complete with modern language models and
recent changes to the post-training stack. In this analogy:</p>
<ul>
<li>Self-supervised learning on vast swaths of internet data makes up
the majority of the cake (especially when viewed in compute spent in
FLOPs),</li>
<li>The beginning of post-training in supervised finetuning (SFT) for
instructions tunes the model to a narrower distribution (along with the
help of chosen examples for RLHF), and</li>
<li>Finally “pure” reinforcement learning (RL) is the cherry on
top.</li>
</ul>
<p>We learn just “a few bits” of information with RL in just a few
training samples. This little bit of reasoning training emerged with
<strong>reasoning models</strong> that use a combination of the
post-training techniques discussed in this book to align preferences
along with RL training on verifiable domains to dramatically increase
capabilities such as reasoning, coding, and mathematics problem
solving.</p>
<p>The training method for these models, Reinforcement Learning with
Verifiable Rewards (RLVR) <span class="citation"
data-cites="lambert2024t">[@lambert2024t]</span>, proceeds very
similarly to RLHF, but it makes the reward model optional in lieu of a
scoring function that returns a positive reward when the answer is
correct and 0 otherwise.</p>
<figure id="fig:rlvr">
<img src="images/rlvr-system.png"
alt="RLVR in the form of an RL feedback loop. Instead of a reward model, we use a verification function." />
<figcaption aria-hidden="true">RLVR in the form of an RL feedback loop.
Instead of a reward model, we use a verification function.</figcaption>
</figure>
<p>The first models to successfully deploy this type of training were
OpenAI’s o1 <span class="citation"
data-cites="openai2024o1">[@openai2024o1]</span> and the open-weight
model DeepSeek R1 <span class="citation"
data-cites="guo2025deepseek">[@guo2025deepseek]</span>. Soon after, the
the entire AI industry prioritized this training process and model
style. The core change here is more of a reallocation of the stages of
training and the priority of different behaviors rather than this type
of RL setup being entirely new. Reasoning models brought an era where
scaling RL training is expected.</p>
<p>As for the type of behavior these models accrue, consider the
following example with DeepSeek V3 0325 versus their reasoning model,
DeepSeek R1, on the query
<code>Write me a short poem about a goldfish</code>. DeepSeek V3 is very
to the point:</p>
<blockquote>
<p>Goldfish Dreams</p>
<p>In a bowl of glass so clear and round, A tiny goldfish swims without
a sound. With scales that gleam like sunlight’s kiss, He drifts along in
watery bliss.</p>
<p>A flick of fin, a flash so bright, Chasing bubbles in the light. Does
he dream of oceans wide? Or is his little world enough—his pride?</p>
</blockquote>
<p>Meanwhile, DeepSeek R1’s experience is different. It first generates
numerous thinking tokens to craft a response before responding to the
user. The answer appears as:</p>
<blockquote>
<p><code>&lt;thinking&gt;</code> Okay, the user wants a short poem about
a goldfish. Let me start by brainstorming some goldfish-related imagery.
Goldfish are often in bowls, right? So maybe start with the setting.
Words like “gilded bowl” or “crystal globe” could work.</p>
<p>Next, think about the movement of the goldfish. They swim smoothly,
so words like “dart”, “glide”, or “swirl” come to mind. Maybe describe
their fins as “silken” to emphasize delicacy.</p>
<p>Then, consider the environment around the goldfish. Sunlight through
water creates shimmering effects. Words like “sunbeams dance” or
“ripples spin” might capture that. Including elements like castles or
pebbles adds a playful touch.</p>
<p>Themes? Maybe the contrast between the goldfish’s small world and its
perception of freedom. The bowl as a universe, tiny but complete. Ending
with a sense of mystery or silent observation could give depth. Need to
keep it concise, maybe four stanzas with a rhyme scheme. Let me check
the rhythm and flow. Avoid clichés but keep it accessible. Alright,
let’s piece it together. <code>&lt;/thinking&gt;</code></p>
<p>Gilded Dreams in Liquid Glass</p>
<p>A gilded bowl, a crystal globe, Where tiny tides in silence flow— A
flash of orange, a whispered robe, You dart through sunlit indigo.</p>
<p>Your world is small, yet boundless too— Each pebble, castle, weed
unfurled. The surface bends the sky to blue, And ripples spin your
liquid world.</p>
<p>You glide through myths we’ll never know, A silent monarch, veiled in
mist. Do dreams drift where the bubbles go? Or memories in amethyst?</p>
<p>We watch you trace your endless sphere— A universe within a tear.</p>
</blockquote>
<p>The tokens in the <code>&lt;thinking&gt;</code> tags is the model’s
reasoning. For more complex problems the reasoning stage can take
thousands of tokens before producing an answer. So, long-context
language models were a prerequisite to advanced reasoning behavior, but
that is not the focus of this chapter.</p>
<p>The core intuition for <em>how this training works</em> is that for a
given model, we repeat the following cycle:</p>
<ol type="1">
<li>Sample multiple answers to multiple questions,</li>
<li>Take gradient steps towards the answers that are correct, and</li>
<li>Repeat, revisiting the same data.</li>
</ol>
<p>Remarkably, this extremely simple approach (when done with a careful
distribution of data and stable training infrastructure) helps the
models learn by revisiting the same questions again and again. Even more
remarkable is that the improvements on these training questions
generalize to questions and (some) domains the models have never
seen!</p>
<p>This simple approach allows the models to lightly search over
behavior space and the RL algorithm increases the likelihood of
behaviors that are correlated with correct answers.</p>
<h2 data-number="1.1" id="why-does-rl-work-now"><span
class="header-section-number">1.1</span> Why Does RL Work Now?</h2>
<p>Despite many, many takes that “RL doesn’t work yet” <span
class="citation" data-cites="irpan2018deep">[@irpan2018deep]</span> or
paper’s detailing deep reproducibility issues with RL <span
class="citation"
data-cites="henderson2018deep">[@henderson2018deep]</span>, the field
overcame it to find high-impact applications. The takeoff of RL-focused
training on language models indicates steps in many fundamental issues
for the research area, including:</p>
<ul>
<li><p><strong>Stability of RL can be solved</strong>: For its entire
existence, the limiting factor on RL’s adoption has been stability. This
manifests in two ways. First, the learning itself can be fickle and not
always work. Second, the training itself is known to be more brittle
than standard language model training and more prone to loss spikes,
crashes, etc. Countless releases are using this style of RL training and
substantial academic uptake has occurred. The technical barriers to
entry on RL are at an all time low.</p></li>
<li><p><strong>Open-source versions already “exist”</strong>: Many tools
already exist for training language models with RLVR and related
techniques. Examples include TRL <span class="citation"
data-cites="vonwerra2022trl">[@vonwerra2022trl]</span>, Open Instruct
[lambert2024t], veRL <span class="citation"
data-cites="sheng2024hybridflow">[@sheng2024hybridflow]</span>, and
OpenRLHF <span class="citation"
data-cites="hu2024openrlhf">[@hu2024openrlhf]</span>, where many of
these are building on optimizations from earlier in the arc of RLHF and
post-training. The accessibility of tooling is enabling a large uptake
of research that’ll likely soon render this chapter out of
date.</p></li>
</ul>
<p>Multiple resources point to RL training for reasoning only being
viable on leading models coming out from about 2024 onwards, indicating
that a certain level of underlying capability was needed in the models
before reasoning training was possible.</p>
<h2 data-number="1.2" id="rl-training-vs.-inference-time-scaling"><span
class="header-section-number">1.2</span> RL Training vs. Inference Time
Scaling</h2>
<p>Training with Reinforcement Learning to illicit reasoning behaviors
and performance on verifiable domains is closely linked to the ideas of
inference time scaling. Inference-time scaling, also called test-time
scaling, is the general class of methods that use more computational
power at inference in order to perform better at a downstream tasks.
Methods for inference-time scaling were studied before the release of
DeepSeek R1 and OpenAI’s o1, which both massively popularized investment
in RL training specifically. Examples include value-guided sampling
<span class="citation" data-cites="liu2023don">[@liu2023don]</span> or
repeated random sampling with answer extraction <span class="citation"
data-cites="brown2024large">[@brown2024large]</span>.</p>
<p>RL training is a short path to inference time scaling laws being
used, but in the long-term we will have more methods for eliciting the
inference-time tradeoffs we need for best performance. Training models
heavily with RL changes them so that they generate more tokens per
response in a way that is strongly correlated with downstream
performance. This is a substantial shift from the length-bias seen in
early RLHF systems <span class="citation"
data-cites="singhal2023long">[@singhal2023long]</span>, where the human
preference training had a side effect of increasing response rate for
marginal gains on preference rankings.</p>
<p>Downstream of the RL trained models there are many methods being
explored to continue to push the limits of reasoning and inference-time
compute. These are largely out of the scope of this book due to their
rapidly evolving nature, but they include distilling reasoning behavior
from a larger RL trained model to a smaller model via instruction tuning
<span class="citation"
data-cites="muennighoff2025s1">[@muennighoff2025s1]</span>, composing
more inference calls <span class="citation"
data-cites="chen2024more">[@chen2024more]</span>, and more. What is
important here is the correlation between downstream performance and an
increase in the number of tokens generated – otherwise it is just wasted
energy.</p>
<h2 data-number="1.3"
id="the-future-beyond-reasoning-of-reinforcement-finetuning"><span
class="header-section-number">1.3</span> The Future (Beyond Reasoning)
of Reinforcement Finetuning</h2>
<p>In many domains, these new flavors of RLVR and reinforcement
finetuning are much more aligned with the goals of developers by being
focused on performance rather than behavior. Standard finetuning APIs
generally use a parameter-efficient finetuning method such as LoRA with
supervised finetuning on instructions. Developers pass in prompts and
completions and the model is tuned to match that by updating model
parameters to match the completions, which increases the prevalence of
features from your data in the models generations.</p>
<p>Reinforcement finetuning is focused on matching answers. Given
queries and correct answers, RFT helps the model learn to get the
correct answers. While standard instruction tuning is done with 1 or 2
epochs of loss updates over the data, reinforcement finetuning gets its
name by doing hundreds or thousands of epochs over the same few data
points to give the model time to learn new behaviors. This can be viewed
as reinforcing positive behaviors that would work sparingly in the base
model version into robust behaviors after RFT.</p>
<p><strong>The scope of RL training for language models continues to
grow</strong>: The biggest takeaway from o1 and R1 on a fundamental
scientific level was that we have even more ways to train language
models to potentially valuable behaviors. The more open doors that are
available to researchers and engineers, the more optimism we should have
about AI’s general trajectory.</p>
</body>
</html>
