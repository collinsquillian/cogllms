<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>01-introduction</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction" id="toc-introduction"><span
class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="#what-does-rlhf-do" id="toc-what-does-rlhf-do"><span
class="toc-section-number">1.1</span> What Does RLHF Do?</a></li>
<li><a href="#an-intuition-for-post-training"
id="toc-an-intuition-for-post-training"><span
class="toc-section-number">1.2</span> An Intuition for
Post-Training</a></li>
<li><a href="#how-we-got-here" id="toc-how-we-got-here"><span
class="toc-section-number">1.3</span> How We Got Here</a></li>
<li><a href="#scope-of-this-book" id="toc-scope-of-this-book"><span
class="toc-section-number">1.4</span> Scope of This Book</a>
<ul>
<li><a href="#chapter-summaries" id="toc-chapter-summaries"><span
class="toc-section-number">1.4.1</span> Chapter Summaries</a></li>
<li><a href="#target-audience" id="toc-target-audience"><span
class="toc-section-number">1.4.2</span> Target Audience</a></li>
<li><a href="#how-to-use-this-book" id="toc-how-to-use-this-book"><span
class="toc-section-number">1.4.3</span> How to Use This Book</a></li>
<li><a href="#about-the-author" id="toc-about-the-author"><span
class="toc-section-number">1.4.4</span> About the Author</a></li>
</ul></li>
<li><a href="#future-of-rlhf" id="toc-future-of-rlhf"><span
class="toc-section-number">1.5</span> Future of RLHF</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="introduction"><span
class="header-section-number">1</span> Introduction</h1>
<p>Reinforcement learning from Human Feedback (RLHF) is a technique used
to incorporate human information into AI systems. RLHF emerged primarily
as a method to solve hard to specify problems. Its early applications
were often in control problems and other traditional domains for
reinforcement learning (RL). RLHF became most known through the release
of ChatGPT and the subsequent rapid development of large language models
(LLMs) and other foundation models.</p>
<p>The basic pipeline for RLHF involves three steps. First, a language
model that can follow user questions must be trained (see Chapter 9).
Second, human preference data must be collected for the training of a
reward model of human preferences (see Chapter 7). Finally, the language
model can be optimized with an RL optimizer of choice, by sampling
generations and rating them with respect to the reward model (see
Chapter 3 and 11). This book details key decisions and basic
implementation examples for each step in this process.</p>
<p>RLHF has been applied to many domains successfully, with complexity
increasing as the techniques have matured. Early breakthrough
experiments with RLHF were applied to deep reinforcement learning <span
class="citation"
data-cites="christiano2017deep">[@christiano2017deep]</span>,
summarization <span class="citation"
data-cites="stiennon2020learning">[@stiennon2020learning]</span>,
following instructions <span class="citation"
data-cites="ouyang2022training">[@ouyang2022training]</span>, parsing
web information for question answering <span class="citation"
data-cites="nakano2021webgpt">[@nakano2021webgpt]</span>, and
“alignment” <span class="citation"
data-cites="bai2022training">[@bai2022training]</span>. A summary of the
early RLHF recipes is shown below in <span class="citation"
data-cites="fig:rlhf-basic">@fig:rlhf-basic</span>.</p>
<figure id="fig:rlhf-basic">
<img src="images/rlhf-basic.png"
alt="A rendition of the early, three stage RLHF process with SFT, a reward model, and then optimization." />
<figcaption aria-hidden="true">A rendition of the early, three stage
RLHF process with SFT, a reward model, and then
optimization.</figcaption>
</figure>
<p>In modern language model training, RLHF is one component of
post-training. Post-training is a more complete set of techniques and
best-practices to make language models more useful for downstream tasks
<span class="citation" data-cites="lambert2024t">[@lambert2024t]</span>.
Post-training can be summarized as using three optimization methods:</p>
<ol type="1">
<li>Instruction / Supervised Finetuning (IFT/SFT), where we teach
formatting and for base of instruction following abilities. This is
largely about learning <em>features</em> in language.</li>
<li>Preference Finetuning (PreFT), where we align to human preferences
(and get smaller bump in capabilities at the same time). This is largely
about <em>style</em> of language and subtle human preferences that are
hard to quantify.</li>
<li>Reinforcement Finetuning (RFT). The newest type of post-training
that boosts performance on verifiable domains.</li>
</ol>
<p>This book focuses on the second area, <strong>preference
finetuning</strong>, which has more complexity than instruction tuning
and is far more established than Reinforcement Finetuning. That being
said, RLHF colloquially <em>is</em> what led to modern post-training.
Soon after the release of ChatGPT, RLHF encompassed all of
post-training. The foundations of RLHF involve far more than preferences
alone and this book provides introductions to all the related
topics.</p>
<h2 data-number="1.1" id="what-does-rlhf-do"><span
class="header-section-number">1.1</span> What Does RLHF Do?</h2>
<p>The biggest question around RLHF, yet one that is still hard to
answer, is “What does RLHF training offer models?” The core role of this
book, beyond teaching the techniques for doing RLHF, is to distill
intuition as to <em>why</em> RLHF is crucial to modern AI models. In
recent years, language models have shifted from academic experiments
studied in the purview of benchmarks to general purpose technology. RLHF
is at the core of this transition.</p>
<p>To start, let us consider how a language model at the end of
autoregressive pretraining answers a query. A strong model, Llama 3.1
405B, will continue from “The president of the united states in 2006
was”:</p>
<blockquote>
<p>George w bush, the governor of Florida in 2006 was Jeb bush, and John
McCain was an arizona senator in 2006 - who later lost to obama.
September 1 – U.S. President Bush signs an executive order to provide
more options for faith-based organizations when delivering social
services that are paid for with federal funds. January 1 – The current
deadline set by United States Congress in the Unlawful Internet Gambling
Enforcement Act, aka UIGEA. search: Amendments to the 1961 International
Convention for the Protection of New Varieties of Plants require plant
breeders’ rights include farmer’s privilege. 2009 was a common year
starting on Thursday of the Gregorian calendar, the 2009th year of the
Common Era (ce) and Anno Domini (ad) designations, the 9th year of the
3rd millennium, the 9th year of the 21st century, and the 10th and last
year of the 2000s decade.</p>
</blockquote>
<p>What is clear is that this model is completing the sentence and
adding other common internet metadata.</p>
<p>The most compelling view of how RLHF works is to think of how
<em>style</em> applies to interactions you have with language models.
The style, or format, of information presented is crucial to how it is
learned. This has always been the case for examples such as coursework,
but is normally applied in the background and not considered
directly.</p>
<p>Continuing the above example, a model trained with RLHF (and other
post-training tools), would answer the question very differently. Asking
Tülu 3 405B the same question “The president of the united states in
2006 was,” is says concisely:</p>
<blockquote>
<p>George W. Bush was the president of the United States in 2006. He
served two terms in office, from January 20, 2001, to January 20,
2009.</p>
</blockquote>
<p>Modern research has established RLHF as a general method to integrate
subtle stylistic and related behavioral features into the models.
Compared to other techniques for post-training, such as instruction
finetuning, RLHF generalizes far better across domains <span
class="citation"
data-cites="kirk2023understanding">[@kirk2023understanding]</span> <span
class="citation" data-cites="chu2025sft">[@chu2025sft]</span> – helping
create effective general purpose models.</p>
<p>Intuitively, this can be seen in how the optimization techniques are
applied. Instruction finetuning is training the model to predict the
next certain token when the text preceding is close to examples it has
seen. It is optimizing the model to more regularly output specific
features in text. This is a per-token update.</p>
<p>RLHF on the other hand tunes the responses on the response level
rather than looking at the next token specifically. Additionally, it is
telling the model what a <em>better</em> response looks like, rather
than a specific response it should learn. RLHF also shows a model which
type of response it should avoid, i.e. negative feedback. The training
to achieve this is often called a <em>contrastive</em> loss function and
is referenced throughout this book.</p>
<p>While this flexibility is a major advantage of RLHF, it comes with
implementation challenges. Largely, these center on <em>how to control
the optimization.</em> As we will cover in this book, implementing RLHF
often requires training a reward model, of which best practices are not
strongly established and depend on the area of application. With this,
the optimization itself is prone to <em>over-optimization</em> because
our reward signal is at best a proxy objective, requiring
regularization. With these limitations, effective RLHF requires a strong
starting point, so RLHF cannot be a solution to every problem alone and
needs to be approached in a broader lens of post-training.</p>
<p>Due to this complexity, implementing RLHF is far more costly than
simple instruction finetuning and can come with unexpected challenges
such as length bias <span class="citation"
data-cites="singhal2023long">[@singhal2023long]</span> <span
class="citation"
data-cites="park2024disentangling">[@park2024disentangling]</span>. For
projects where performance matters, RLHF is established as being crucial
to achieving a strong finetuned model, but it is more expensive in
compute, data costs, and time.</p>
<h2 data-number="1.2" id="an-intuition-for-post-training"><span
class="header-section-number">1.2</span> An Intuition for
Post-Training</h2>
<p>Here’s a simple analogy for how so many gains can be made on mostly
the same base model.</p>
<p>The intuition I’ve been using to understand the potential of
post-training is called the elicitation interpretation of post-training,
where all we are doing is extracting and amplifying valuable behaviors
in the base model.</p>
<p>Consider Formula 1 (F1), most of the teams show up to the beginning
of the year with a new chassis and engine. Then, they spend all year on
aerodynamics and systems changes (of course, it is a minor
oversimplification), and can dramatically improve the performance of the
car. The best F1 teams improve way more during a season than
chassis-to-chassis.</p>
<p>The same is true for post-training. The best post-training teams
extract a ton of performance in a very short time frame. The set of
techniques is everything after the end of most of pretraining. It
includes “mid-training” like annealing / high-quality end of
pre-training web data, instruction tuning, RLVR, preference-tuning, etc.
A good example is our change from the first version of OLMoE Instruct to
the second — the post-training evaluation average from 35 to 48 without
touching the majority of pretraining <span class="citation"
data-cites="ai2_olmoe_ios_2025">[@ai2_olmoe_ios_2025]</span>.</p>
<p>Then, when you look at models such as GPT-4.5, you can see this as a
way more dynamic and exciting base for OpenAI to build onto. We also
know that bigger base models can absorb far more diverse changes than
their smaller counterparts.</p>
<p>This is to say that scaling also allows post-training to move faster.
Of course, to do this, you need the infrastructure to train the models.
This is why all the biggest companies are still building gigantic
clusters.</p>
<p>This theory folds in with the reality that the majority of gains
users are seeing are from post-training because it implies that there is
more latent potential in a model pretraining on the internet than we can
teach the model simply — such as by passing certain narrow samples in
repeatedly during early types of post-training (i.e. only instruction
tuning).</p>
<p>Another name for this theory is the Superficial Alignment Hypothesis,
coined in the paper LIMA: Less is More for Alignment <span
class="citation" data-cites="zhou2023lima">[@zhou2023lima]</span>. This
paper is getting some important intuitions right but for the wrong
reasons in the big picture. The authors state:</p>
<blockquote>
<p>A model’s knowledge and capabilities are learnt almost entirely
during pretraining, while alignment teaches it which subdistribution of
formats should be used when interacting with users. If this hypothesis
is correct, and alignment is largely about learning style, then a
corollary of the Superficial Alignment Hypothesis is that one could
sufficiently tune a pretrained language model with a rather small set of
examples [Kirstain et al., 2021].</p>
</blockquote>
<p>All of the successes of deep learning should have taught you a deeply
held belief that scaling data is important to performance. Here, the
major difference is that the authors are discussing alignment and style,
the focus of academic post-training at the time. With a few thousand
samples for instruction finetuning, you can change a model substantially
and improve a narrow set of evaluations, such as AlpacaEval, MT Bench,
ChatBotArena, and the likes. These do not always translate to more
challenging capabilities, which is why Meta wouldn’t train its Llama
Chat models on just this dataset. Academic results have lessons, but
need to be interpreted carefully if you are trying to understand the big
picture of the technological arc.</p>
<p>What this paper is showing is that you can change models
substantially with a few samples. We knew this, and it is important to
the short-term adaptation of new models, but their argument for
performance leaves the casual readers with the wrong lessons.</p>
<p>If we change the data, the impact could be far higher on the model’s
performance and behavior, but it is far from “superficial.” Base
language models today (with no post-training) can be trained on some
mathematics problems with reinforcement learning, learn to output a full
chain of thought reasoning, and then score higher on a full suite of
reasoning evaluations like BigBenchHard, Zebra Logic, AIME, etc.</p>
<p>The superficial alignment hypothesis is wrong for the same reason
that people who think RLHF and post-training are just for vibes are
still wrong. This was a field-wide lesson we had to overcome in 2023
(one many AI observers are still rooted in). Post-training has far
outgrown that, and we are coming to see that the style of models
operates on top of behavior — such as the now popular long chain of
thought.</p>
<h2 data-number="1.3" id="how-we-got-here"><span
class="header-section-number">1.3</span> How We Got Here</h2>
<p>Why does this book make sense now? How much still will change?</p>
<p>Post-training, the craft of eliciting powerful behaviors from a raw
pretrained language model, has gone through many seasons and moods since
the release of ChatGPT that sparked the renewed interest in RLHF. In the
era of Alpaca <span class="citation"
data-cites="alpaca">[@alpaca]</span>, Vicuna <span class="citation"
data-cites="vicuna2023">[@vicuna2023]</span>, <span class="citation"
data-cites="koala_blogpost_2023">[@koala_blogpost_2023]</span>, and
Dolly <span class="citation"
data-cites="DatabricksBlog2023DollyV1">[@DatabricksBlog2023DollyV1]</span>,
a limited number of human datapoints with extended synthetic data in the
style of Self-Instruct were used to normally fine-tune the original
LLaMA to get similar behavior to ChatGPT. The benchmark for these early
models was fully vibes (and human evaluation) as we were all so
captivated by the fact that these small models can have such impressive
behaviors across domains. It was justified excitement.</p>
<p>Open post-training was moving faster, releasing more models, and
making more noise than its closed counterparts. Companies were
scrambling, e.g. DeepMind merging with Google or being started, and
taking time to follow it up. There are phases of open recipes surging
and then lagging behind.</p>
<p>The era following Alpaca et al., the first lag in open recipes, was
one defined by skepticism and doubt on reinforcement learning from human
feedback (RLHF), the technique OpenAI highlighted as crucial to the
success of the first ChatGPT. Many companies doubted that they needed to
do RLHF. A common phrase – “instruction tuning is enough for alignment”
– was so popular then that it still holds heavy weight today despite
heavy obvious pressures against it.</p>
<p>This doubt of RLHF lasted, especially in the open where groups cannot
afford data budgets on the order of $100K to $1M. The companies that
embraced it early ended up winning out. Anthropic published extensive
research on RLHF through 2022 and is now argued to have the best
post-training <span class="citation"
data-cites="askell2021general">[@askell2021general]</span> <span
class="citation" data-cites="bai2022training">[@bai2022training]</span>
<span class="citation"
data-cites="bai2022constitutional">[@bai2022constitutional]</span>. The
delta between open groups, struggling to reproduce, or even knowing
basic closed techniques, is a common theme.</p>
<p>The first shift in open alignment methods and post-training was the
story of Direct Preference Optimization (DPO) <span class="citation"
data-cites="rafailov2024direct">[@rafailov2024direct]</span>. The DPO
paper, posted in May of 2023, didn’t have any clearly impactful models
trained with it going through the fall of 2023. This changed with the
releases of a few breakthrough DPO models – all contingent on finding a
better, lower, learning rate. Zephyr-Beta <span class="citation"
data-cites="tunstall2023zephyr">[@tunstall2023zephyr]</span>, Tülu 2
<span class="citation"
data-cites="ivison2023camels">[@ivison2023camels]</span>, and many other
models showed that the DPO era of post-training had begun. Chris Manning
literally thanked me for “saving DPO.” This is how fine the margins are
on evolutions of best practices with leading labs being locked down.
Open post-training was cruising again.</p>
<p>Preference-tuning was something you needed to do to meet the table
stakes of releasing a good model since late 2023. The DPO era continued
through 2024, in the form of never-ending variants on the algorithm, but
we were very far into another slump in open recipes. Open post-training
recipes had saturated the extent of knowledge and resources
available.<br />
A year after Zephyr and Tulu 2, the same breakout dataset, UltraFeedback
is arguably still state-of-the-art for preference tuning in open recipes
<span class="citation"
data-cites="cui2023ultrafeedback">[@cui2023ultrafeedback]</span>.</p>
<p>At the same time, the Llama 3.1 <span class="citation"
data-cites="dubey2024llama">[@dubey2024llama]</span> and Nemotron 4 340B
<span class="citation"
data-cites="adler2024nemotron">[@adler2024nemotron]</span> reports gave
us substantive hints that large-scale post-training is much more complex
and impactful. The closed labs are doing full post-training – a large
multi-stage process of instruction tuning, RLHF, prompt design, etc. –
where academic papers are just scratching the surface. Tülu 3
represented a comprehensive, open effort to build the foundation of
future academic post-training research <span class="citation"
data-cites="lambert2024t">[@lambert2024t]</span>.</p>
<p>Today, post-training is a complex process involving the
aforementioned training objectives applied in various orders in order to
target specific capabilities. This book is designed to give a platform
to understand all of these techniques, and in coming years the best
practices for how to interleave them will emerge.</p>
<p>The primary areas of innovation in post-training are now in
reinforcement finetuning, reasoning training, and related ideas. This
newer methods build extensively on the infrastructure and ideas of RLHF,
but are evolving far faster. This book is written to capture the first
stable literature for RLHF after its initial period of rapid change.</p>
<h2 data-number="1.4" id="scope-of-this-book"><span
class="header-section-number">1.4</span> Scope of This Book</h2>
<p>This book hopes to touch on each of the core steps of doing canonical
RLHF implementations. It will not cover all the history of the
components nor recent research methods, just techniques, problems, and
trade-offs that have been proven to occur again and again.</p>
<h3 data-number="1.4.1" id="chapter-summaries"><span
class="header-section-number">1.4.1</span> Chapter Summaries</h3>
<p>This book has the following chapters:</p>
<h4 data-number="1.4.1.1" id="introductions"><span
class="header-section-number">1.4.1.1</span> Introductions</h4>
<p>Reference material useful throughout the book.</p>
<ol type="1">
<li>Introduction: Overview of RLHF and what this book provides.</li>
<li>Seminal (Recent) Works: Key models and papers in the history of RLHF
techniques.</li>
<li>Definitions: Mathematical definitions for RL, language modeling, and
other ML techniques leveraged in this book.</li>
</ol>
<h4 data-number="1.4.1.2" id="problem-setup-context"><span
class="header-section-number">1.4.1.2</span> Problem Setup &amp;
Context</h4>
<p>Context for the big picture problem RLHF is trying to solve.</p>
<ol start="4" type="1">
<li>RLHF Training Overview: How the training objective for RLHF is
designed and basics of understanding it.</li>
<li>What are preferences?: Why human preference data is needed to fuel
and understand RLHF.</li>
<li>Preference Data: How preference data is collected for RLHF.</li>
</ol>
<h4 data-number="1.4.1.3" id="optimization-tools"><span
class="header-section-number">1.4.1.3</span> Optimization Tools</h4>
<p>The suite of techniques used to optimize language models to align
them to human preferences. This is a serial presentation of the
techniques one can use to solve the problems proposed in the previous
chapters.</p>
<ol start="7" type="1">
<li>Reward Modeling: Training reward models from preference data that
act as an optimization target for RL training (or for use in data
filtering).</li>
<li>Regularization: Tools to constrain these optimization tools to
effective regions of the parameter space.</li>
<li>Instruction Tuning: Adapting language models to the question-answer
format.</li>
<li>Rejection Sampling: A basic technique for using a reward model with
instruction tuning to align models.</li>
<li>Policy Gradients: The core RL techniques used to optimize reward
models (and other signals) throughout RLHF.</li>
<li>Direct Alignment Algorithms: Algorithms that optimize the RLHF
objective direction from pairwise preference data rather than learning a
reward model first.</li>
</ol>
<h4 data-number="1.4.1.4" id="advanced-tbd"><span
class="header-section-number">1.4.1.4</span> Advanced (TBD)</h4>
<p>Newer RLHF techniques and discussions that are not clearly
established, but are important to current generations of models.</p>
<ol start="13" type="1">
<li>Constitutional AI and AI Feedback</li>
<li>Reasoning and Reinforcement Finetuning</li>
<li>Synthetic Data</li>
<li>Evaluation</li>
</ol>
<h4 data-number="1.4.1.5" id="open-questions-tbd"><span
class="header-section-number">1.4.1.5</span> Open Questions (TBD)</h4>
<p>Fundamental problems and discussions for the long-term evolution of
how RLHF is used.</p>
<ol start="16" type="1">
<li>Over-optimization</li>
<li>Style and Information</li>
</ol>
<h3 data-number="1.4.2" id="target-audience"><span
class="header-section-number">1.4.2</span> Target Audience</h3>
<p>This book is intended for audiences with entry level experience with
language modeling, reinforcement learning, and general machine learning.
It will not have exhaustive documentation for all the techniques, but
just those crucial to understanding RLHF.</p>
<h3 data-number="1.4.3" id="how-to-use-this-book"><span
class="header-section-number">1.4.3</span> How to Use This Book</h3>
<p>This book was largely created because there were no canonical
references for important topics in the RLHF workflow. The contributions
of this book are supposed to give you the minimum knowledge needed to
try a toy implementation or dive into the literature. This is
<em>not</em> a comprehensive textbook, but rather a quick book for
reminders and getting started. Additionally, given the web-first nature
of this book, it is expected that there are minor typos and somewhat
random progressions – please contribute by fixing bugs or suggesting
important content on <a
href="https://github.com/natolambert/rlhf-book">GitHub</a>.</p>
<h3 data-number="1.4.4" id="about-the-author"><span
class="header-section-number">1.4.4</span> About the Author</h3>
<p>Dr. Nathan Lambert is a RLHF researcher contributing to the open
science of language model fine-tuning. He has released many models
trained with RLHF, their subsequent datasets, and training codebases in
his time at the Allen Institute for AI (Ai2) and HuggingFace. Examples
include <a
href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta">Zephyr-Beta</a>,
<a href="https://huggingface.co/allenai/tulu-2-dpo-70b">Tulu 2</a>, <a
href="https://huggingface.co/allenai/OLMo-7B-Instruct">OLMo</a>, <a
href="https://github.com/huggingface/trl">TRL</a>, <a
href="https://github.com/allenai/open-instruct">Open Instruct</a>, and
many more. He has written extensively on RLHF, including <a
href="https://www.interconnects.ai/t/rlhf">many blog posts</a> and <a
href="https://scholar.google.com/citations?hl=en&amp;user=O4jW7BsAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">academic
papers</a>.</p>
<h2 data-number="1.5" id="future-of-rlhf"><span
class="header-section-number">1.5</span> Future of RLHF</h2>
<p>With the investment in language modeling, many variations on the
traditional RLHF methods emerged. RLHF colloquially has become
synonymous with multiple overlapping approaches. RLHF is a subset of
preference fine-tuning (PreFT) techniques, including Direct Alignment
Algorithms (See Chapter 12). RLHF is the tool most associated with rapid
progress in “post-training” of language models, which encompasses all
training after the large-scale autoregressive training on primarily web
data. This textbook is a broad overview of RLHF and its directly
neighboring methods, such as instruction tuning and other implementation
details needed to set up a model for RLHF training.</p>
<p>As more successes of fine-tuning language models with RL emerge, such
as OpenAI’s o1 reasoning models, RLHF will be seen as the bridge that
enabled further investment of RL methods for fine-tuning large base
models.</p>
<!-- This is the first paragraph of the introduction chapter.

## First: Images

This is the first subsection. Please, admire the gloriousnes of this seagull:

![A cool seagull.](images/seagull.png)

A bigger seagull:

![A cool big seagull.](images/seagull.png){ width=320px }

## Second: Tables

This is the second subsection.


Please, check [First: Images] subsection.

Please, check [this](#first-images) subsection.

| Index | Name |
| ----- | ---- |
| 0     | AAA  |
| 1     | BBB  |
| ...   | ...  |

Table: This is an example table.

## Third: Equations

Formula example: $\mu = \sum_{i=0}^{N} \frac{x_i}{N}$

Now, full size:

$$\mu = \sum_{i=0}^{N} \frac{x_i}{N}$$

And a code sample:

```rb
def hello_world
  puts "hello world!"
end

hello_world
```

Check these unicode characters: ǽß¢ð€đŋμ

## Fourth: Cross references

These cross references are disabled by default. To enable them, check the
_[Cross references](https://github.com/wikiti/pandoc-book-template#cross-references)_
section on the README.md file.

Here's a list of cross references:

- Check @fig:seagull.
- Check @tbl:table.
- Check @eq:equation.

![A cool seagull](images/seagull.png){#fig:seagull}

$$ y = mx + b $$ {#eq:equation}

| Index | Name |
| ----- | ---- |
| 0     | AAA  |
| 1     | BBB  |
| ...   | ...  |

Table: This is an example table. {#tbl:table} -->
</body>
</html>
