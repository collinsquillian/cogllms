<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>17-over-optimization</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#over-optimization" id="toc-over-optimization"><span
class="toc-section-number">1</span> Over Optimization</a>
<ul>
<li><a href="#qualitative-behavioral-over-optimization"
id="toc-qualitative-behavioral-over-optimization"><span
class="toc-section-number">1.1</span> Qualitative (behavioral)
over-optimization</a>
<ul>
<li><a href="#managing-proxy-objectives"
id="toc-managing-proxy-objectives"><span
class="toc-section-number">1.1.1</span> Managing proxy
objectives</a></li>
<li><a href="#llama-2-and-too-much-rlhf"
id="toc-llama-2-and-too-much-rlhf"><span
class="toc-section-number">1.1.2</span> Llama 2 and “too much
RLHF”</a></li>
<li><a href="#an-aside-on-undercooking-rlhf"
id="toc-an-aside-on-undercooking-rlhf"><span
class="toc-section-number">1.1.3</span> An aside on “undercooking”
RLHF</a></li>
</ul></li>
<li><a href="#quantitative-over-optimization"
id="toc-quantitative-over-optimization"><span
class="toc-section-number">1.2</span> Quantitative
over-optimization</a></li>
<li><a href="#misalignment" id="toc-misalignment"><span
class="toc-section-number">1.3</span> Misalignment</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="over-optimization"><span
class="header-section-number">1</span> Over Optimization</h1>
<p>In the RLHF literature and discourse, there are three directions that
over-optimization can emerge:</p>
<ol type="1">
<li><strong>Quantitative research</strong> on the technical notion of
over-optimization of reward,</li>
<li><strong>Qualitative observations</strong> that “overdoing” RLHF can
result in worse models.</li>
<li><strong>Misalignment</strong> where overdoing RLHF or related
techniques can make a language model behave against its design.</li>
</ol>
<p>This chapter provides a cursory introduction to both. We begin with
the latter, qualitative, because it motivates the problem to study
further.</p>
<h2 data-number="1.1"
id="qualitative-behavioral-over-optimization"><span
class="header-section-number">1.1</span> Qualitative (behavioral)
over-optimization</h2>
<p><em>Note: This section draws on two <a
href="https://www.interconnects.ai/p/llama-2-part-2">blog</a> <a
href="https://www.interconnects.ai/p/specifying-objectives-in-rlhf">posts</a>
from Interconnects.ai. It can also be viewed as an “objective mismatch”
<span class="citation"
data-cites="lambert2023alignment">[@lambert2023alignment]</span> <span
class="citation"
data-cites="lambert2020objective">[@lambert2020objective]</span>.</em></p>
<h3 data-number="1.1.1" id="managing-proxy-objectives"><span
class="header-section-number">1.1.1</span> Managing proxy
objectives</h3>
<p>The thing about RLHF that should be more obvious is that we don’t
have a good reward function for chatbots. RLHF has been driven into the
forefront because of its impressive performance at making chatbots a bit
better to use (from both eliminating bad stuff and a bit of adding
capabilities), which is entirely governed by a proxy objective —
thinking that the rewards measured from human labelers in a controlled
setting mirror those desires of downstream users. Post-training
generally has emerged to include training on explicitly verifiable
rewards, but standard learning from preferences alone also improves
performance on domains such as mathematical reasoning and coding.</p>
<p>The proxy reward in RLHF is the score returned by a trained reward
model to the RL algorithm itself because it is known to only be at best
correlated with chatbot performance <span class="citation"
data-cites="schulman2023proxy">[@schulman2023proxy]</span>. Therefore,
it’s been shown that applying too much optimization power to the RL part
of the algorithm will actually decrease the usefulness of the final
language model. And over-optimization, put simply by John, is “when
optimizing the proxy objective causes the true objective to get better,
then get worse.” A curve where the training loss goes up, slowly levels
off, then goes down. This is different from overfitting, where the model
accuracy keeps getting better on the training distribution.
Over-optimization of a proxy reward is much more subtle (and linked to
the current <a
href="https://www.interconnects.ai/t/evaluation">evaluation fog</a> in
NLP, where it’s hard to know which models are actually “good”).</p>
<p>The general notion captured by this reasoning follows from Goodhart’s
law, which is colloquially the notion that “When a measure becomes a
target, it ceases to be a good measure.” This adage is derived from
Goodhart’s writing <span class="citation"
data-cites="goodhart1984problems">[@goodhart1984problems]</span>:</p>
<blockquote>
<p>Any observed statistical regularity will tend to collapse once
pressure is placed upon it for control purposes.</p>
</blockquote>
<p>The insight here builds on the fact that we have optimizations we are
probably incorrectly using ML losses as ground truths in these complex
systems. In reality, the loss functions we use are designed (and
theoretically motivated for) local optimizations. The global use of them
is resulting in challenges with the RLHF proxy objective.</p>
<p>Common signs of over-optimization in early chat models emerged
as:</p>
<ul>
<li>“As an AI language model…”</li>
<li>“Certainly!…”</li>
<li>Repetitiveness, hedging, …</li>
<li>Self-doubt, sycophancy <span class="citation"
data-cites="sharma2023towards">[@sharma2023towards]</span>, and over
apologizing</li>
<li>Over refusals (more below)</li>
</ul>
<p>Technically, it is an open question on which types of error in the
training process result in these failures. Many sources of error exist
<span class="citation"
data-cites="schulman2023proxy">[@schulman2023proxy]</span>:
Approximation error from reward models not being able to fit to
preferences, estimation error from overfitting during training the RM,
optimization error in training the language model policy, etc. This
points to a fundamental question as to the limits of optimization the
intents of data contractors relative to what downstream users want.</p>
<p>A potential solution is that <em>implicit</em> feedback will be
measured from users of chatbots and models to tune performance. Implicit
feedback is actions taken by the user, such as re-rolling an output,
closing the tab, or writing an angry message that indicates the quality
of the previous response. The challenge here, and with most optimization
changes to RLHF, is that there’s a strong risk of losing stability when
making the reward function more specific. RL, as a strong optimizer, is
increasingly likely to exploit the reward function when it is a smooth
surface (and not just pairwise human values). The expected solution to
this is that future RLHF will be trained with both pairwise preference
data and additional steering loss functions. There are also a bunch of
different loss functions that can be used to better handle pairwise
data, such as Mallow’s model <span class="citation"
data-cites="lu2011learning">[@lu2011learning]</span> or Plackett-Luce
<span class="citation"
data-cites="liu2019learning">[@liu2019learning]</span>.</p>
<h3 data-number="1.1.2" id="llama-2-and-too-much-rlhf"><span
class="header-section-number">1.1.2</span> Llama 2 and “too much
RLHF”</h3>
<h3 data-number="1.1.3" id="an-aside-on-undercooking-rlhf"><span
class="header-section-number">1.1.3</span> An aside on “undercooking”
RLHF</h3>
<p>As training practices for language models have matured, there are
also prominent cases where strong models do not have an amount of
post-training that most users expect, resulting in models that are
harder to use than their evaluation scores would suggest.</p>
<p>TODO add references to minimax model? See tweets etc?</p>
<h2 data-number="1.2" id="quantitative-over-optimization"><span
class="header-section-number">1.2</span> Quantitative
over-optimization</h2>
<p>KL is the primary metric,</p>
<p>Put simply, the solution that will most likely play out is to use
bigger models. Bigger models have more room for change in the very
under-parameterized setting of a reward model (sample efficient part of
the equation), so are less impacted. DPO may not benefit from this as
much, the direct optimization will likely change sample efficiency one
way or another.</p>
<p><span class="citation"
data-cites="gao2023scaling">[@gao2023scaling]</span></p>
<h2 data-number="1.3" id="misalignment"><span
class="header-section-number">1.3</span> Misalignment</h2>
<p>Consequences <span class="citation"
data-cites="zhuang2020consequences">[@zhuang2020consequences]</span> or
sycophancy <span class="citation"
data-cites="sharma2023towards">[@sharma2023towards]</span></p>
</body>
</html>
