<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>12-direct-alignment</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/usr/share/javascript/mathjax/MathJax.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#direct-alignment-algorithms"
id="toc-direct-alignment-algorithms"><span
class="toc-section-number">1</span> Direct Alignment Algorithms</a>
<ul>
<li><a href="#direct-preference-optimization-dpo"
id="toc-direct-preference-optimization-dpo"><span
class="toc-section-number">1.1</span> Direct Preference Optimization
(DPO)</a>
<ul>
<li><a href="#how-dpo-works" id="toc-how-dpo-works"><span
class="toc-section-number">1.1.1</span> How DPO Works</a></li>
<li><a href="#dpo-derivation" id="toc-dpo-derivation"><span
class="toc-section-number">1.1.2</span> DPO Derivation</a></li>
</ul></li>
<li><a href="#numerical-concerns-weaknesses-and-alternatives"
id="toc-numerical-concerns-weaknesses-and-alternatives"><span
class="toc-section-number">1.2</span> Numerical Concerns, Weaknesses,
and Alternatives</a></li>
<li><a href="#implementation-considerations"
id="toc-implementation-considerations"><span
class="toc-section-number">1.3</span> Implementation
Considerations</a></li>
<li><a href="#daas-vs.-rl-online-vs.-offline-data"
id="toc-daas-vs.-rl-online-vs.-offline-data"><span
class="toc-section-number">1.4</span> DAAs vs. RL: Online vs. Offline
Data</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="direct-alignment-algorithms"><span
class="header-section-number">1</span> Direct Alignment Algorithms</h1>
<p>Direct Alignment Algorithms (DAAs) allow one to update models to
solve the same RLHF objective without ever training an intermediate
reward model or using reinforcement learning optimizers. The most
prominent DAA and one that catalyzed an entire academic movement of
aligning language models is Direct Preference Optimization (DPO) <span
class="citation"
data-cites="rafailov2024direct">[@rafailov2024direct]</span>. At its
core, DPO is using gradient ascent to solve the same constrained RLHF
objective. Since its release in May of 2023, after a brief delay where
the community figured out the right data and hyperparameters to use DPO
with (specifically, surprisingly low learning rates), many popular
models have used DPO or its variants, from Zephyr-<span
class="math inline">\(\beta\)</span> kickstarting it in October of 2023
<span class="citation"
data-cites="tunstall2023zephyr">[@tunstall2023zephyr]</span>, Llama 3
Instruct <span class="citation"
data-cites="dubey2024llama">[@dubey2024llama]</span>, Tülu 2 <span
class="citation"
data-cites="ivison2023camels">[@ivison2023camels]</span> and 3 <span
class="citation" data-cites="lambert2024t">[@lambert2024t]</span>,
Nemotron 4 340B <span class="citation"
data-cites="adler2024nemotron">[@adler2024nemotron]</span>, and others.
Technically, Sequence Likelihood Calibration (SLiC-HF) was released
first <span class="citation"
data-cites="zhao2023slic">[@zhao2023slic]</span>, but it did not catch
on due to a combination of luck and effectiveness.</p>
<p>The most impactful part of DPO and DAAs is lowering the barrier of
entry to experimenting with language model post-training.</p>
<h2 data-number="1.1" id="direct-preference-optimization-dpo"><span
class="header-section-number">1.1</span> Direct Preference Optimization
(DPO)</h2>
<p>Here we explain intuitions for how it works and re-derive the core
equations fully.</p>
<h3 data-number="1.1.1" id="how-dpo-works"><span
class="header-section-number">1.1.1</span> How DPO Works</h3>
<p>DPO at a surface level is directly optimizing a policy to solve the
RLHF objective. The loss function for this, which we will revisit below
in the derivations, is a pairwise relationship of log-probabilities. The
loss function derived from a Bradley-Terry reward model follows:</p>
<p><span class="math display">\[ \mathcal{L}_{\text{DPO}}(\pi_\theta;
\pi_{\text{ref}}) = -\mathbb{E}_{(x, y_c, y_r) \sim \mathcal{D}}\left[
\log \sigma\left( \beta \log \frac{\pi_{\theta}(y_c \mid
x)}{\pi_{\text{ref}}(y_c \mid x)} - \beta \log \frac{\pi_{\theta}(y_r
\mid x)}{\pi_{\text{ref}}(y_r \mid x)} \right) \right] \]</span>
{#eq:dpo_core}</p>
<p>This relies on the implicit reward for DPO training that replaces
using an external reward model, which is a log-ratio of
probabilities:</p>
<p><span class="math display">\[r(x, y) = \beta  \log \frac{\pi_r(y \mid
x)}{\pi_{\text{ref}}(y \mid x)}\]</span> {#eq:dpo_reward}</p>
<p>This comes from deriving the Bradley-Terry reward with respect to an
optimal policy (shown in <span class="citation"
data-cites="eq:dpo_opt_policy">@eq:dpo_opt_policy</span>), as shown in
the Bradley-Terry model section. Essentially, the implicit reward model
shows “the probability of human preference data in terms of the optimal
policy rather than the reward model.”</p>
<p>Let us consider the loss shown in <span class="citation"
data-cites="eq:dpo_core">@eq:dpo_core</span>. The learning process is
decreasing the loss. Here, the loss will be lower when the log-ratio of
the chosen response is bigger than the log-ratio of the rejected
response (normalized by the reference model). In practice, this is a sum
of log-probabilities of the model across the sequence of tokens in the
data presented. Hence, DPO is increasing the delta in probabilities
between the chosen and rejected responses.</p>
<p>With the reward in <span class="citation"
data-cites="eq:dpo_reward">@eq:dpo_reward</span>, we can write the
gradient of the loss to further interpret what is going on:</p>
<p><span
class="math display">\[\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};
\pi_{\text{ref}}) = -\beta \mathbb{E}_{(x, y_c, y_r)\sim
\mathcal{D}}\left[ \sigma\left(r_{\theta}(x, y_r) - r_{\theta}(x,
y_c)\right) \left(\nabla_{\theta}\log \pi(y_c \mid x) -
\nabla_{\theta}\log \pi(y_r \mid x)\right) \right] \]</span>
{#eq:dpo_gradient}</p>
<p>Here, the gradient solves the above objective by doing the
following:</p>
<ul>
<li>The first term within the sigmoid function, <span
class="math inline">\(\sigma(\cdot)\)</span>, creates a weight of the
parameter update from 0 to 1 that is higher when the reward estimate is
incorrect. When the rejected sample is preferred over the chosen, the
weight update should be larger!</li>
<li>Second, the terms in the inner brackets <span
class="math inline">\([\cdot]\)</span> increases the likelihood of the
chosen response <span class="math inline">\(y_c\)</span> and decreases
the likelihood of the rejected <span
class="math inline">\(y_r\)</span>.</li>
<li>These terms are weighted by <span
class="math inline">\(\beta\)</span>, which controls how the update
balances ordering the completions correctly relative to the KL
distance.</li>
</ul>
<p>The core intuition is that DPO is “fitting an implicit reward model
whose corresponding optimal policy can be extracted in a closed form”
(thanks to gradient ascent and our ML tools). What is often
misunderstood is that DPO is learning a reward model at its core, hence
the subtitle of the paper <em>Your Language Model is Secretly a Reward
Model.</em> It is easy to confuse this with the DPO objective training a
policy directly, hence studying the derivations below are good for a
complete understanding.</p>
<p>With the implicit reward model learning, DPO is generating an optimal
solution to the RLHF objective given the data in the dataset and the
specific KL constraint in the objective <span
class="math inline">\(\beta\)</span>. Here, DPO solves for the exact
policy given a specific KL distance because the generations are not
online as in policy gradient algorithms – a core difference from the RL
methods for preference tuning. In many ways, this makes the <span
class="math inline">\(\beta\)</span> value easier to tune with DPO
relative to online RL methods, but crucially and intuitively the optimal
value depends on the model being trained and the data training it.</p>
<p>At each batch of preference data, composed of many pairs of
completions <span class="math inline">\(y_{chosen} \succ
y_{rejected}\)</span>, DPO takes gradient steps directly towards the
optimal solution. It is far simpler than policy gradient methods.</p>
<figure id="fig:dpo-meme">
<img src="images/dpo_meme.jpeg"
alt="DPO simplicity meme, credit Tom Goldstein." />
<figcaption aria-hidden="true">DPO simplicity meme, credit Tom
Goldstein.</figcaption>
</figure>
<h3 data-number="1.1.2" id="dpo-derivation"><span
class="header-section-number">1.1.2</span> DPO Derivation</h3>
<p>The DPO derivation takes two primary parts. First, the authors show
the form of the policy that optimally solved the RLHF objective used
throughout this book. Next, they show how to arrive at that solution
from pairwise preference data (i.e. a Bradley Terry model).</p>
<h4 data-number="1.1.2.1" id="deriving-the-optimal-rlhf-solution"><span
class="header-section-number">1.1.2.1</span> 1. Deriving the Optimal
RLHF Solution</h4>
<p>To start, we should consider the RLHF optimization objective once
again, here indicating we wish to maximize this quantity:</p>
<p><span class="math display">\[ \max_{\pi} \mathbb{E}_{\tau \sim \pi}
\left[r_\theta(s_t, a_t)\right] -
\beta  \mathcal{D}_{KL}(\pi^{\text{RL}}(\cdot|s_t) \|
\pi^{\text{ref}}(\cdot|s_t)).\]</span> {#eq:rlhf_opt_eq_repeat}</p>
<p>First, let us expand the definition of KL-divergence,</p>
<p><span class="math display">\[\max_{\pi} \mathbb{E}_{x \sim
\mathcal{D}}\mathbb{E}_{y \sim
\pi(y|x)}\left[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]
\]</span> {#eq:dpo_deriv_1}</p>
<p>Next, pull the negative sign out of the difference in brackets. To do
this, split it into two terms:</p>
<p><span class="math display">\[ = \max_{\pi}\left(\mathbb{E}_{x \sim
\mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}[r(x,y)] - \beta\,\mathbb{E}_{x
\sim \mathcal{D}}\mathbb{E}_{y \sim
\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)}\right]\right)
\]</span> {#eq:dpo_deriv_2}</p>
<p>Then, remove the factor of <span class="math inline">\(-1\)</span>
and <span class="math inline">\(\beta\)</span>,</p>
<p><span class="math display">\[ = \min_{\pi}\left(-\mathbb{E}_{x \sim
\mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}[r(x,y)] + \beta\,\mathbb{E}_{x
\sim \mathcal{D}}\mathbb{E}_{y \sim
\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}\right]\right)
\]</span> {#eq:dpo_deriv_3}</p>
<p>Divide by <span class="math inline">\(\beta\)</span> and
recombine:</p>
<p><span class="math display">\[ = \min_{\pi}\left(\mathbb{E}_{x \sim
\mathcal{D}}\mathbb{E}_{y \sim \pi(y|x)}\left[
\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y)
\right]\right) \]</span> {#eq:dpo_deriv_4}</p>
<p>Next, we must introduce a partition function, <span
class="math inline">\(Z(x)\)</span>:</p>
<p><span class="math display">\[ Z(x) = \sum_y
\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right) \]</span>
{#eq:dpo_partition}</p>
<p>The partition function acts as a normalization factor over the
reference policy, summing over all possible responses <span
class="math inline">\(y\)</span> to a prompt <span
class="math inline">\(x\)</span>. With this substituted in, we obtain
our intermediate transformation:</p>
<p><span class="math display">\[
\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}
- \log Z(x)\right] \]</span> {#eq:dpo_deriv_5}</p>
<p>To see how this is obtained, consider the internal part of the
optimization in brackets of <span class="citation"
data-cites="eq:dpo_deriv_4">@eq:dpo_deriv_4</span>:</p>
<p><span class="math display">\[
\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y)
\]</span> {#eq:dpo_deriv_6}</p>
<p>Then, add <span class="math inline">\(\log Z(x) - \log Z(x)\)</span>
to both sides:</p>
<p><span class="math display">\[ =
\log\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} - \frac{1}{\beta}r(x,y) +
\log Z(x) - \log Z(x) \]</span> {#eq:dpo_deriv_7}</p>
<p>Then, we group the terms:</p>
<p><span class="math display">\[ = \left( \log
\frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} + \log Z(x) \right) - \log Z(x) -
\frac{1}{\beta}r(x,y) \]</span> {#eq:dpo_deriv_8}</p>
<p>With <span class="math inline">\(\log(x) + \log(y) = \log(x\cdot
y)\)</span> (and moving <span class="math inline">\(Z\)</span> to the
denominator), we get:</p>
<p><span class="math display">\[ = \log
\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)}- \log Z(x) -
\frac{1}{\beta}r(x,y) \]</span> {#eq:dpo_deriv_9}</p>
<p>Next, we expand <span
class="math inline">\(\frac{1}{\beta}r(x,y)\)</span> to <span
class="math inline">\(\log \exp \frac{1}{\beta}r(x,y)\)</span> and do
the same operation to get <span class="citation"
data-cites="eq:dpo_deriv_5">@eq:dpo_deriv_5</span>. With this
optimization form, we need to actually solve for the optimal policy
<span class="math inline">\(\pi^*\)</span>. To do so, let us consider
the above optimization as a KL distance:</p>
<p><span class="math display">\[
\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{D}_\text{KL}
\left(\pi(y|x)||\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
\right) - \log Z(x)\right] \]</span> {#eq:dpo_deriv_10}</p>
<p>Since the partition function <span
class="math inline">\(Z(x)\)</span> does not depend on the final answer,
we can ignore it. This leaves us with just the KL distance between our
policy we are learning and a form relating the partition, <span
class="math inline">\(\beta\)</span>, reward, and reference policy. The
Gibb’s inequality tells this is minimized at a distance of 0, only when
the two quantities are equal! Hence, we get an optimal policy:</p>
<p><span class="math display">\[ \pi^*(y|x) = \pi(y|x) =
\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
\]</span> {#eq:dpo_opt_policy}</p>
<h4 data-number="1.1.2.2"
id="deriving-dpo-objective-for-bradley-terry-models"><span
class="header-section-number">1.1.2.2</span> 2. Deriving DPO Objective
for Bradley Terry Models</h4>
<p>To start, recall from Chapter 7 on Reward Modeling and Chapter 6 on
Preference Data that a Bradley-Terry model of human preferences is
formed as:</p>
<p><span class="math display">\[p^*(y_1 \succ y_2 \mid x) =
\frac{\exp\left(r^*(x,y_1)\right)}{\exp\left(r^*(x,y_1)\right) +
\exp\left(r^*(x, y_2)\right)} \]</span> {#eq:bradley_terry_dpo}</p>
<p>By manipulating <span class="citation"
data-cites="eq:dpo_opt_policy">@eq:dpo_opt_policy</span> by taking the
logarithm of both sides and performing some algebra, one can obtain the
DPO reward as follows:</p>
<p><span class="math display">\[r^*(x, y) = \beta \log \frac{\pi^*(y
\mid x)}{\pi_{\text{ref}}(y \mid x)} + \beta \log Z(x)\]</span>
{#eq:dpo_reward_full}</p>
<p>We then can substitute the reward into the Bradley-Terry equation
shown in <span class="citation"
data-cites="eq:bradley_terry_dpo">@eq:bradley_terry_dpo</span> to
obtain:</p>
<p><span class="math display">\[p^*(y_1 \succ y_2 \mid x) =
\frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1
\mid x)} + \beta \log Z(x)\right)}
{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid
x)} + \beta \log Z(x)\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid
x)}{\pi_{\text{ref}}(y_2 \mid x)} + \beta \log Z(x)\right)} \]</span>
{#eq:dpo_loss_deriv0}</p>
<p>By decomposing the exponential expressions from <span
class="math inline">\(e^{a+b}\)</span> to <span
class="math inline">\(e^a e^b\)</span> and then cancelling out the terms
<span class="math inline">\(e^{\log(Z(x))}\)</span>, this simplifies
to:</p>
<p><span class="math display">\[p^*(y_1 \succ y_2 \mid x) =
\frac{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1
\mid x)}\right)}
{\exp\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid
x)}\right) + \exp\left(\beta \log \frac{\pi^*(y_2 \mid
x)}{\pi_{\text{ref}}(y_2 \mid x)}\right)} \]</span>
{#eq:dpo_loss_deriv1}</p>
<p>Then, multiply the numerator and denominator by <span
class="math inline">\(\exp\left(-\beta \log \frac{\pi^*(y_1 \mid
x)}{\pi_{\text{ref}}(y_1 \mid x)}\right)\)</span> to obtain:</p>
<p><span class="math display">\[p^*(y_1 \succ y_2 \mid x) = \frac{1}{1 +
\exp\left(\beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid
x)} - \beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1 \mid
x)}\right)} \]</span> {#eq:dpo_loss_deriv2}</p>
<p>Finally, with the definition of a sigmoid function as <span
class="math inline">\(\sigma(x) = \frac{1}{1+e^{-x}}\)</span>, we
obtain:</p>
<p><span class="math display">\[p^*(y_1 \succ y_2 \mid x) =
\sigma\left(\beta \log \frac{\pi^*(y_1 \mid x)}{\pi_{\text{ref}}(y_1
\mid x)} - \beta \log \frac{\pi^*(y_2 \mid x)}{\pi_{\text{ref}}(y_2 \mid
x)}\right) \]</span> {#eq:dpo_loss_deriv3}</p>
<p>This is the loss function for DPO, as shown in <span class="citation"
data-cites="eq:dpo_core">@eq:dpo_core</span>. The DPO paper has an
additional derivation for the objective under a Plackett-Luce Model,
which is far less used in practice <span class="citation"
data-cites="rafailov2024direct">[@rafailov2024direct]</span>.</p>
<h4 data-number="1.1.2.3"
id="deriving-the-bradley-terry-dpo-gradient"><span
class="header-section-number">1.1.2.3</span> 3. Deriving the Bradley
Terry DPO Gradient</h4>
<p>We used the DPO gradient shown in <span class="citation"
data-cites="eq:dpo_gradient">@eq:dpo_gradient</span> to explain
intuitions for how the model learns. To derive this, we must take the
gradient of <span class="citation"
data-cites="eq:dpo_loss_deriv3">@eq:dpo_loss_deriv3</span> with respect
to the model parameters.</p>
<p><span
class="math display">\[\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};
\pi_{\text{ref}}) =
-\nabla_{\theta}\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[ \log
\sigma\left(\beta \log
\frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \log
\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}\right)\right]
\]</span> {#eq:dpo_grad_0}</p>
<p>To start, this can be rewritten. We know that the derivative of a
sigmoid function <span class="math inline">\(\frac{d}{dx} \sigma(x) =
\sigma(x)(1-\sigma(x))\)</span>, the derivative of logarithm <span
class="math inline">\(\frac{d}{dx} \log x = \frac{1}{x}\)</span>, and
properties of sigmoid <span
class="math inline">\(\sigma(-x)=1-\sigma(x)\)</span>, so we can
reformat the above equation.</p>
<p>First, define the expression inside the sigmoid as <span
class="math inline">\(u=\beta \log
\frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \log
\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}\)</span>. Then, we
have</p>
<p><span
class="math display">\[\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})
= -\mathbb{E}_{(x, y_c, y_r)\sim
\mathcal{D}}\left[\frac{\sigma&#39;(u)}{\sigma(u)}\nabla_{\theta}u\right]
\]</span> {#eq:dpo_grad_2}</p>
<p>Expanding this and using the above expressions for sigmoid and
logarithms results in the gradient introduced earlier:</p>
<p><span class="math display">\[
-\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[\beta\sigma\left(\beta\log\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}
-
\beta\log\frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)}\right)\left[\nabla_{\theta}\log\pi(y_c|x)-\nabla_{\theta}\log\pi(y_r|x)\right]\right]
\]</span> {#eq:dpo_grad_3}</p>
<h2 data-number="1.2"
id="numerical-concerns-weaknesses-and-alternatives"><span
class="header-section-number">1.2</span> Numerical Concerns, Weaknesses,
and Alternatives</h2>
<p>Many variants of the DPO algorithm have been proposed to address
weaknesses of DPO. For example, without rollouts where a reward model
can rate generations, DPO treats every pair of preference data with
equal weight. In reality, as seen in Chapter 6 on Preference Data, there
are many ways of capturing preference data with a richer label than
binary. Multiple algorithms have been proposed to re-balance the
optimization away from treating each pair equally.</p>
<ul>
<li><strong>REgression to RElative REward Based RL (REBEL)</strong> adds
signal from a reward model, as a margin between chosen and rejected
responses, rather than solely the pairwise preference data to more
accurately solve the RLHF problem <span class="citation"
data-cites="gao2024rebel">[@gao2024rebel]</span>.</li>
<li><strong>Conservative DPO (cDPO) and Identity Preference Optimization
(IPO)</strong> address the overfitting by assuming noise in the
preference data. cDPO assumes N percent of the data is incorrectly
labelled <span class="citation"
data-cites="rafailov2024direct">[@rafailov2024direct]</span> and IPO
changes the optimization to soften probability of preference rather than
optimize directly from a label <span class="citation"
data-cites="azar2024general">[@azar2024general]</span>. Practically, IPO
changes the preference probability to a nonlinear function, moving away
from the Bradley-Terry assumption, with <span
class="math inline">\(\Psi(q) =
\log\left(\frac{q}{1-q}\right)\)</span>.</li>
<li><strong>DPO with an offset (ODPO)</strong> “requires the difference
between the likelihood of the preferred and dispreferred response to be
greater than an offset value” <span class="citation"
data-cites="amini2024direct">[@amini2024direct]</span> – do not treat
every data pair equally, but this can come at the cost of a more
difficult labeling environment.</li>
</ul>
<p>Some variants to DPO attempt to either improve the learning signal by
making small changes to the loss or make the application more efficient
by reducing memory usage.</p>
<ul>
<li><strong>Odds Ratio Policy Optimization (ORPO)</strong> directly
updates the policy model with a pull towards the chosen response,
similar to the instruction finetuning loss, with a small penalty on the
chosen response <span class="citation"
data-cites="hong2024reference">[@hong2024reference]</span>. This change
of loss function removes the need for a reference model, simplifying the
setup. The best way to view ORPO is DPO inspired, rather than a DPO
derivative.</li>
<li><strong>Simple Preference Optimization SimPO</strong> makes a minor
change to the DPO optimization, by averaging the log-probabilities
rather than summing them (SimPO) or adding length normalization, to
improve performance <span class="citation"
data-cites="meng2025simpo">[@meng2025simpo]</span>.</li>
</ul>
<figure id="fig:dpo_issue">
<img src="images/dpo_displacement.png" class="center"
alt="Sketch of preference displacement in DPO." />
<figcaption aria-hidden="true">Sketch of preference displacement in
DPO.</figcaption>
</figure>
<p>One of the core issues <em>apparent</em> in DPO is that the
optimization drives only to increase the margin between the probability
of the chosen and rejected responses. Numerically, the model reduces the
probability of both the chosen and rejected responses, but the
<em>rejected response is reduced by a greater extent</em> as shown in
<span class="citation" data-cites="fig:dpo_issue">@fig:dpo_issue</span>.
Intuitively, it is not clear how this generalizes, but work has posited
that it increases the probability of unaddressed for behaviors <span
class="citation"
data-cites="razin2024unintentional">[@razin2024unintentional]</span>
<span class="citation"
data-cites="ren2024learning">[@ren2024learning]</span>. Simple
methods—such as Cal-DPO <span class="citation"
data-cites="xiao2024cal">[@xiao2024cal]</span>, which adjusts the
optimization process, and AlphaPO <span class="citation"
data-cites="gupta2025alphapo">[@gupta2025alphapo]</span>, which modifies
the reward shape—mitigate this <strong>preference displacement</strong>.
In practice, the exact impact of this is not well known, but points are
a potential reason why online methods can outperform vanilla DPO.</p>
<p>The largest other reason that is posited for DPO-like methods to have
a lower ceiling on performance than online (RL based) RLHF methods is
that the training signal comes from completions from previous or other
models. Online variants that sample generations from the model,
e.g. <strong>Online DPO</strong> <span class="citation"
data-cites="guo2024direct">[@guo2024direct]</span>, even with regular
reward model relabelling of newly created creations
<strong>Discriminator-Guided DPO</strong> (D2PO) <span class="citation"
data-cites="singhal2024d2po">[@singhal2024d2po]</span>, alleviate these
by generating new completions for the prompt and incorporating a
preference signal at training time.</p>
<p>There is a long list of other DAA variants, such as Direct Nash
Optimization (DNO) <span class="citation"
data-cites="rosset2024direct">[@rosset2024direct]</span> or Binary
Classifier Optimization (BCO) <span class="citation"
data-cites="jung2024binary">[@jung2024binary]</span>, but the choice of
algorithm is far less important than the initial model and the data used
<span class="citation" data-cites="lambert2024t">[@lambert2024t]</span>
<span class="citation"
data-cites="zhao2024rainbowpo">[@zhao2024rainbowpo]</span> <span
class="citation"
data-cites="gorbatovski2025differences">[@gorbatovski2025differences]</span>.</p>
<h2 data-number="1.3" id="implementation-considerations"><span
class="header-section-number">1.3</span> Implementation
Considerations</h2>
<p>DAAs such as DPO are implemented very differently than policy
gradient optimizers. The DPO loss, taken from the original
implementation, largely can be summarized as follows <span
class="citation"
data-cites="rafailov2024direct">[@rafailov2024direct]</span>:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pi_logratios <span class="op">=</span> policy_chosen_logps <span class="op">-</span> policy_rejected_logps</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ref_logratios <span class="op">=</span> reference_chosen_logps <span class="op">-</span> reference_rejected_logps</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> pi_logratios <span class="op">-</span> ref_logratios  <span class="co"># also known as h_{\pi_\theta}^{y_w,y_l}</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> <span class="op">-</span>F.logsigmoid(beta <span class="op">*</span> logits)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>chosen_rewards <span class="op">=</span> beta <span class="op">*</span> (policy_chosen_logps <span class="op">-</span> reference_chosen_logps).detach()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>rejected_rewards <span class="op">=</span> beta <span class="op">*</span> (policy_rejected_logps <span class="op">-</span> reference_rejected_logps).detach()</span></code></pre></div>
<p>This can be used in standard language model training stacks as this
information is already collated during the forward pass of a model (with
the addition of a reference model).</p>
<p>In most ways, this is simpler and an quality of life improvement, but
also they offer a different set of considerations.</p>
<ol type="1">
<li><strong>KL distance is static</strong>: In DPO and other algorithms,
the KL distance is set explicitly by the <span
class="math inline">\(\beta\)</span> parameter that balances the
distance penalty to the optimization. This is due to the fact that DPO
takes gradient steps towards the <em>optimal</em> solution to the RLHF
objective given the data – it steps exactly to the solution set by the
<span class="math inline">\(\beta\)</span> term. On the other hand, RL
based optimizers take steps based on the batch and recent data.</li>
<li><strong>Caching log-probabilities</strong>: Simple implementations
of DPO do the forward passes for the policy model and reference models
at the same time for conveniences with respect to the loss function.
Though, this doubles the memory used and results in increased GPU usage.
To avoid this, one can compute the log-probabilities of the reference
model over the training dataset first, then reference it when computing
the loss and updating the parameters per batch, reducing the peak memory
usage by 50%.</li>
</ol>
<h2 data-number="1.4" id="daas-vs.-rl-online-vs.-offline-data"><span
class="header-section-number">1.4</span> DAAs vs. RL: Online vs. Offline
Data</h2>
<p>Broadly, the argument boils down to one question: Do we need the
inner workings of reinforcement learning, with value functions, policy
gradients, and all, to align language models with RLHF? This, like most
questions phrased this way, is overly simplistic. Of course, both
methods are well-established, but it is important to illustrate where
the fundamental differences and performance manifolds lie.</p>
<p>Multiple reports have concluded that policy-gradient based and RL
methods outperform DPO and its variants. The arguments take different
forms, from training models with different algorithms but controlled
data<span class="citation"
data-cites="ivison2024unpacking">[@ivison2024unpacking]</span> <span
class="citation" data-cites="xu2024dpo">[@xu2024dpo]</span> or studying
the role of on-policy data within the RL optimization loop <span
class="citation"
data-cites="tajwar2024preference">[@tajwar2024preference]</span>. In all
of these cases, DPO algorithms are a hair behind.</p>
<p>Even with this performance delta, DAA are still used extensively in
leading models due to its simplicity. DAAs provide a controlled
environment where iterations on training data and other configurations
can be made rapidly, and given that data is often far more important
than algorithms, using DPO can be fine.</p>
<p>With the emergence of reasoning models that are primarily trained
with RL, further investment will return to using RL for
preference-tuning, which in the long-term will improve the robustness of
RL infrastructure and cement this margin between DAAs and RL for
optimizing from human feedback.</p>
</body>
</html>
