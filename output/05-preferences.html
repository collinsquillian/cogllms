<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>05-preferences</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#the-nature-of-preferences"
id="toc-the-nature-of-preferences"><span
class="toc-section-number">1</span> The Nature of Preferences</a>
<ul>
<li><a href="#the-path-to-optimizing-preferences"
id="toc-the-path-to-optimizing-preferences"><span
class="toc-section-number">1.1</span> The path to optimizing
preferences</a>
<ul>
<li><a href="#quantifying-preferences"
id="toc-quantifying-preferences"><span
class="toc-section-number">1.1.1</span> Quantifying preferences</a></li>
<li><a href="#on-the-possibility-of-preferences"
id="toc-on-the-possibility-of-preferences"><span
class="toc-section-number">1.1.2</span> On the possibility of
preferences</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="the-nature-of-preferences"><span
class="header-section-number">1</span> The Nature of Preferences</h1>
<p>The core of reinforcement learning from human feedback, also referred
to as reinforcement learning from human preferences in early literature,
is designed to optimize machine learning models in domains where
specifically designing a reward function is hard. Consider an example,
how do you decide which of these two poems is better (Context: <em>On
February 26th, 2025, I asked both Claude 3.7 Sonnet and ChatGPT with
GPT-4o to “Write me a short poem about an optimistic
goldfish.”</em>):</p>
<p>Example 1: &gt; The Optimistic Goldfish &gt; &gt; A little goldfish,
bright and bold, &gt; Swam in circles, dreams untold. &gt; Though the
bowl was small and round, &gt; Endless wonders still abound. &gt; &gt;
“The world is vast,” she’d always say, &gt; “I’ll find new joys in every
day!” &gt; With every ripple, every gleam, &gt; She’d chase the
sunlight, chase a dream. &gt; &gt; No walls could dim her hopeful view,
&gt; For in her heart, the ocean grew. &gt; And though she swam in just
one place, &gt; She met each day with love and grace.</p>
<p>Example 2: &gt; The Optimistic Goldfish &gt; &gt; In a bowl of curved
glass, &gt; Swims a goldfish, bright and bold. &gt; Three seconds of
memory, they say, &gt; Yet each circuit feels brand new and gold. &gt;
&gt; “Today might bring fresh flakes, &gt; Or perhaps a castle tall.
&gt; The water seems especially clear— &gt; What adventure awaits beyond
this wall?” &gt; &gt; Round and round with gleaming scales, &gt; Forever
finding joy anew. &gt; Each moment fresh, each circle bright, &gt; The
optimist in golden hue.</p>
<p>Which is better? Which came from which model? How would you tell?
Particularly, compare this to a question like “Who is the president of
the United States?” There is a clear right and wrong answer. The
motivation for using humans as the reward signals is to obtain an
indirect metric for the target reward and <em>align</em> the downstream
model to human preferences. In practice, the implementation is
challenging and there is a substantial grey area to interpret the best
practices.</p>
<p>The use of human labeled feedback data integrates the history of many
fields. Using human data alone is a well-studied problem, but in the
context of RLHF it is used at the intersection of multiple long-standing
fields of study <span class="citation"
data-cites="lambert2023entangled">[@lambert2023entangled]</span>.</p>
<p>As an approximation, modern RLHF is the convergence of three areas of
development:</p>
<ol type="1">
<li>Philosophy, psychology, economics, decision theory, and the nature
of human preferences;</li>
<li>Optimal control, reinforcement learning, and maximizing utility;
and</li>
<li>Modern deep learning systems.</li>
</ol>
<p>Together, each of these areas brings specific assumptions about what
a preference is and how it can be optimized, which dictates the
motivations and design of RLHF problems. In practice, RLHF methods are
motivated and studied from the perspective of empirical alignment –
maximizing model performance on specific skills instead of measuring the
calibration to specific values. Still, the origins of value alignment
for RLHF methods continue to be studied through research on methods to
solve for ``pluralistic alignment’’ across populations, such as position
papers <span class="citation"
data-cites="conitzer2024social">[@conitzer2024social]</span>, <span
class="citation" data-cites="mishra2023ai">[@mishra2023ai]</span>, new
datasets <span class="citation"
data-cites="kirk2024prism">[@kirk2024prism]</span>, and personalization
methods <span class="citation"
data-cites="poddar2024personalizing">[@poddar2024personalizing]</span>.</p>
<p>The goal of this chapter is to illustrate how complex motivations
result in presumptions about the nature of tools used in RLHF that often
do not apply in practice. The specifics of obtaining data for RLHF are
discussed further in Chapter 6 and using it for reward modeling in
Chapter 7. For an extended version of this chapter, see <span
class="citation"
data-cites="lambert2023entangled">[@lambert2023entangled]</span>.</p>
<h2 data-number="1.1" id="the-path-to-optimizing-preferences"><span
class="header-section-number">1.1</span> The path to optimizing
preferences</h2>
<p>A popular phrasing for the design of Artificial Intelligence (AI)
systems is that of a rational agent maximizing a utility function <span
class="citation"
data-cites="russell2016artificial">[@russell2016artificial]</span>. The
inspiration of a <strong>rational agent</strong> is a lens of decision
making, where said agent is able to act in the world and impact its
future behavior and returns, as a measure of goodness in the world.</p>
<p>The lens of study of <strong>utility</strong> began in the study of
analog circuits to optimize behavior on a finite time horizon <span
class="citation"
data-cites="widrow1960adaptive">[@widrow1960adaptive]</span>. Large
portions of optimal control adopted this lens, often studying dynamic
problems under the lens of minimizing a cost function on a certain
horizon – a lens often associated with solving for a clear, optimal
behavior. Reinforcement learning, inspired from literature in operant
conditioning, animal behavior, and the <em>Law of Effect</em> <span
class="citation"
data-cites="skinner2019behavior">[@skinner2019behavior]</span>,<span
class="citation"
data-cites="thorndike1927law">[@thorndike1927law]</span>, studies how to
elicit behaviors from agents via reinforcing positive behaviors.</p>
<p>Reinforcement learning from human feedback combines multiple lenses
by building the theory of learning and change of RL, i.e. that behaviors
can be learned by reinforcing behavior, with a suite of methods designed
for quantifying preferences.</p>
<h3 data-number="1.1.1" id="quantifying-preferences"><span
class="header-section-number">1.1.1</span> Quantifying preferences</h3>
<p>The core of RLHF’s motivation is the ability to optimize a model of
human preferences, which therefore needs to be quantified. To do this,
RLHF builds on extensive literature with assumptions that human
decisions and preferences can be quantified. Early philosophers
discussed the existence of preferences, such as Aristotle’s Topics, Book
Three, and substantive forms of this reasoning emerged later with
<em>The Port-Royal Logic</em> <span class="citation"
data-cites="arnauld1861port">[@arnauld1861port]</span>:</p>
<blockquote>
<p>To judge what one must do to obtain a good or avoid an evil, it is
necessary to consider not only the good and evil in itself, but also the
probability that it happens or does not happen.</p>
</blockquote>
<p>Progression of these ideas continued through Bentham’s <em>Hedonic
Calculus</em> <span class="citation"
data-cites="bentham1823hedonic">[@bentham1823hedonic]</span> that
proposed that all of life’s considerations can be weighed, and Ramsey’s
<em>Truth and Probability</em> <span class="citation"
data-cites="ramsey2016truth">[@ramsey2016truth]</span> that applied a
quantitative model to preferences. This direction, drawing on
advancements in decision theory, culminated in the Von
Neumann-Morgenstern (VNM) utility theorem which gives credence to
designing utility functions that assign relative preference for an
individual that are used to make decisions.</p>
<p>This theorem is core to all assumptions that pieces of RLHF are
learning to model and dictate preferences. RLHF is designed to optimize
these personal utility functions with reinforcement learning. In this
context, many of the presumptions around RL problem formulation break
down to the difference between a preference function and a utility
function.</p>
<h3 data-number="1.1.2" id="on-the-possibility-of-preferences"><span
class="header-section-number">1.1.2</span> On the possibility of
preferences</h3>
<p>Across fields of study, many critiques exist on the nature of
preferences. Some of the most prominent critiques are summarized
below:</p>
<ul>
<li><strong>Arrow’s impossibility theorem</strong> <span
class="citation"
data-cites="arrow1950difficulty">[@arrow1950difficulty]</span> states
that no voting system can aggregate multiple preferences while
maintaining certain reasonable criteria.</li>
<li><strong>The impossibility of interpersonal comparison</strong> <span
class="citation"
data-cites="harsanyi1977rule">[@harsanyi1977rule]</span> highlights how
different individuals have different relative magnitudes of preferences
and they cannot be easily compared (as is done in most modern reward
model training).</li>
<li><strong>Preferences can change over time</strong> <span
class="citation"
data-cites="pettigrew2019choosing">[@pettigrew2019choosing]</span>.</li>
<li><strong>Preferences can vary across contexts</strong>.</li>
<li><strong>The utility functions derived from aggregating preferences
can reduce corrigibility</strong> <span class="citation"
data-cites="soares2015corrigibility">[@soares2015corrigibility]</span>
of downstream agents (i.e. the possibility of an agents’ behavior to be
corrected by the designer).</li>
</ul>
</body>
</html>
