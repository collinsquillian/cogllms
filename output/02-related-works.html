<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>02-related-works</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#key-related-works" id="toc-key-related-works"><span
class="toc-section-number">1</span> Key Related Works</a>
<ul>
<li><a href="#origins-to-2018-rl-on-preferences"
id="toc-origins-to-2018-rl-on-preferences"><span
class="toc-section-number">1.1</span> Origins to 2018: RL on
Preferences</a></li>
<li><a href="#to-2022-rl-from-human-preferences-on-language-models"
id="toc-to-2022-rl-from-human-preferences-on-language-models"><span
class="toc-section-number">1.2</span> 2019 to 2022: RL from Human
Preferences on Language Models</a></li>
<li><a href="#to-present-chatgpt-era"
id="toc-to-present-chatgpt-era"><span
class="toc-section-number">1.3</span> 2023 to Present: ChatGPT
Era</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="key-related-works"><span
class="header-section-number">1</span> Key Related Works</h1>
<p>In this chapter we detail the key papers and projects that got the
RLHF field to where it is today. This is not intended to be a
comprehensive review on RLHF and the related fields, but rather a
starting point and retelling of how we got to today. It is intentionally
focused on recent work that led to ChatGPT. There is substantial further
work in the RL literature on learning from preferences <span
class="citation" data-cites="wirth2017survey">[@wirth2017survey]</span>.
For a more exhaustive list, you should use a proper survey paper <span
class="citation"
data-cites="kaufmann2023survey">[@kaufmann2023survey]</span>,<span
class="citation"
data-cites="casper2023open">[@casper2023open]</span>.</p>
<h2 data-number="1.1" id="origins-to-2018-rl-on-preferences"><span
class="header-section-number">1.1</span> Origins to 2018: RL on
Preferences</h2>
<p>The field has recently been popularized with the growth of Deep
Reinforcement Learning and has grown into a broader study of the
applications of LLMs from many large technology companies. Still, many
of the techniques used today are deeply related to core techniques from
early literature on RL from preferences.</p>
<p><em>TAMER: Training an Agent Manually via Evaluative
Reinforcement,</em> Proposed a learned agent where humans provided
scores on the actions taken iteratively to learn a reward model <span
class="citation" data-cites="knox2008tamer">[@knox2008tamer]</span>.
Other concurrent or soon after work proposed an actor-critic algorithm,
COACH, where human feedback (both positive and negative) is used to tune
the advantage function <span class="citation"
data-cites="macglashan2017interactive">[@macglashan2017interactive]</span>.</p>
<p>The primary reference, Christiano et al. 2017, is an application of
RLHF applied to preferences between Atari trajectories <span
class="citation"
data-cites="christiano2017deep">[@christiano2017deep]</span>. The work
shows that humans choosing between trajectories can be more effective in
some domains than directly interacting with the environment. This uses
some clever conditions, but is impressive nonetheless. This method was
expanded upon with more direct reward modeling <span class="citation"
data-cites="ibarz2018reward">[@ibarz2018reward]</span>. TAMER was
adapted to deep learning with Deep TAMER just one year later <span
class="citation"
data-cites="warnell2018deep">[@warnell2018deep]</span>.</p>
<p>This era began to transition as reward models as a general notion
were proposed as a method for studying alignment, rather than just a
tool for solving RL problems <span class="citation"
data-cites="leike2018scalable">[@leike2018scalable]</span>.</p>
<h2 data-number="1.2"
id="to-2022-rl-from-human-preferences-on-language-models"><span
class="header-section-number">1.2</span> 2019 to 2022: RL from Human
Preferences on Language Models</h2>
<p>Reinforcement learning from human feedback, also referred to
regularly as reinforcement learning from human preferences in its early
days, was quickly adopted by AI labs increasingly turning to scaling
large language models. A large portion of this work began between GPT-2,
in 2018, and GPT-3, in 2020. The earliest work in 2019, <em>Fine-Tuning
Language Models from Human Preferences</em> has many striking
similarities to modern work on RLHF <span class="citation"
data-cites="ziegler2019fine">[@ziegler2019fine]</span>. Learning reward
models, KL distances, feedback diagrams, etc – just the evaluation
tasks, and capabilities, were different. From here, RLHF was applied to
a variety of tasks. The popular applications were the ones that worked
at the time. Important examples include general summarization <span
class="citation"
data-cites="stiennon2020learning">[@stiennon2020learning]</span>,
recursive summarization of books <span class="citation"
data-cites="wu2021recursively">[@wu2021recursively]</span>, instruction
following (InstructGPT) <span class="citation"
data-cites="ouyang2022training">[@ouyang2022training]</span>,
browser-assisted question-answering (WebGPT) <span class="citation"
data-cites="nakano2021webgpt">[@nakano2021webgpt]</span>, supporting
answers with citations (GopherCite) <span class="citation"
data-cites="menick2022teaching">[@menick2022teaching]</span>, and
general dialogue (Sparrow) <span class="citation"
data-cites="glaese2022improving">[@glaese2022improving]</span>.</p>
<p>Aside from applications, a number of seminal papers defined key areas
for the future of RLHF, including those on:</p>
<ol type="1">
<li>Reward model over-optimization <span class="citation"
data-cites="gao2023scaling">[@gao2023scaling]</span>: The ability for RL
optimizers to over-fit to models trained on preference data,</li>
<li>Language models as a general area of study for alignment <span
class="citation"
data-cites="askell2021general">[@askell2021general]</span>, and</li>
<li>Red teaming <span class="citation"
data-cites="ganguli2022red">[@ganguli2022red]</span> – the process of
assessing safety of a language model.</li>
</ol>
<p>Work continued on refining RLHF for application to chat models.
Anthropic continued to use it extensively for early versions of Claude
<span class="citation"
data-cites="bai2022training">[@bai2022training]</span> and early RLHF
open-source tools emerged <span class="citation"
data-cites="ramamurthy2022reinforcement">[@ramamurthy2022reinforcement]</span>,<span
class="citation"
data-cites="havrilla-etal-2023-trlx">[@havrilla-etal-2023-trlx]</span>,<span
class="citation"
data-cites="vonwerra2022trl">[@vonwerra2022trl]</span>.</p>
<h2 data-number="1.3" id="to-present-chatgpt-era"><span
class="header-section-number">1.3</span> 2023 to Present: ChatGPT
Era</h2>
<p>The announcement of ChatGPT was very clear about the role of RLHF in
its training <span class="citation"
data-cites="openai2022chatgpt">[@openai2022chatgpt]</span>:</p>
<blockquote>
<p>We trained this model using Reinforcement Learning from Human
Feedback (RLHF), using the same methods as InstructGPT⁠, but with slight
differences in the data collection setup.</p>
</blockquote>
<p>Since then RLHF has been used extensively in leading language models.
It is well known to be used in Anthropic’s Constitutional AI for Claude
<span class="citation"
data-cites="bai2022constitutional">[@bai2022constitutional]</span>,
Meta’s Llama 2 <span class="citation"
data-cites="touvron2023llama">[@touvron2023llama]</span> and Llama 3
<span class="citation"
data-cites="dubey2024llama">[@dubey2024llama]</span>, Nvidia’s Nemotron
<span class="citation"
data-cites="adler2024nemotron">[@adler2024nemotron]</span>, Ai2’s Tülu 3
<span class="citation" data-cites="lambert2024t">[@lambert2024t]</span>,
and more.</p>
<p>Today, RLHF is growing into a broader field of preference fine-tuning
(PreFT), including new applications such as process reward for
intermediate reasoning steps <span class="citation"
data-cites="lightman2023let">[@lightman2023let]</span>, direct alignment
algorithms inspired by Direct Preference Optimization (DPO) <span
class="citation"
data-cites="rafailov2024direct">[@rafailov2024direct]</span>, learning
from execution feedback from code or math <span class="citation"
data-cites="kumar2024training">[@kumar2024training]</span>,<span
class="citation" data-cites="singh2023beyond">[@singh2023beyond]</span>,
and other online reasoning methods inspired by OpenAI’s o1 <span
class="citation" data-cites="openai2024o1">[@openai2024o1]</span>.</p>
</body>
</html>
