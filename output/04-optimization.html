<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>04-optimization</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/usr/share/javascript/mathjax/MathJax.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#training-overview" id="toc-training-overview"><span
class="toc-section-number">1</span> Training Overview</a>
<ul>
<li><a href="#problem-formulation" id="toc-problem-formulation"><span
class="toc-section-number">1.1</span> Problem Formulation</a></li>
<li><a href="#manipulating-the-standard-rl-setup"
id="toc-manipulating-the-standard-rl-setup"><span
class="toc-section-number">1.2</span> Manipulating the Standard RL
Setup</a></li>
<li><a href="#optimization-tools" id="toc-optimization-tools"><span
class="toc-section-number">1.3</span> Optimization Tools</a></li>
<li><a href="#rlhf-recipe-example" id="toc-rlhf-recipe-example"><span
class="toc-section-number">1.4</span> RLHF Recipe Example</a></li>
<li><a href="#finetuning-and-regularization"
id="toc-finetuning-and-regularization"><span
class="toc-section-number">1.5</span> Finetuning and
Regularization</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="training-overview"><span
class="header-section-number">1</span> Training Overview</h1>
<h2 data-number="1.1" id="problem-formulation"><span
class="header-section-number">1.1</span> Problem Formulation</h2>
<p>The optimization of reinforcement learning from human feedback (RLHF)
builds on top of the standard RL setup. In RL, an agent takes actions,
<span class="math inline">\(a\)</span>, sampled from a policy, <span
class="math inline">\(\pi\)</span>, with respect to the state of the
environment, <span class="math inline">\(s\)</span>, to maximize reward,
<span class="math inline">\(r\)</span> <span class="citation"
data-cites="sutton2018reinforcement">[@sutton2018reinforcement]</span>.
Traditionally, the environment evolves with respect to a transition or
dynamics function <span
class="math inline">\(p(s_{t+1}|s_t,a_t)\)</span>. Hence, across a
finite episode, the goal of an RL agent is to solve the following
optimization:</p>
<p><span class="math display">\[J(\pi) = \mathbb{E}_{\tau \sim \pi}
\left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right],\]</span>
{#eq:rl_opt}</p>
<p>where <span class="math inline">\(\gamma\)</span> is a discount
factor from 0 to 1 that balances the desirability of near- versus
future-rewards. Multiple methods for optimizing this expression are
discussed in Chapter 11.</p>
<figure id="fig:rl">
<img src="images/rl.png" class="center" width="320"
alt="Standard RL loop" />
<figcaption aria-hidden="true">Standard RL loop</figcaption>
</figure>
<p>A standard illustration of the RL loop is shown in <span
class="citation" data-cites="fig:rl">@fig:rl</span> and how it compares
to <span class="citation" data-cites="fig:rlhf">@fig:rlhf</span>.</p>
<h2 data-number="1.2" id="manipulating-the-standard-rl-setup"><span
class="header-section-number">1.2</span> Manipulating the Standard RL
Setup</h2>
<p>There are multiple core changes from the standard RL setup to that of
RLHF:</p>
<ol type="1">
<li>Switching from a reward function to a reward model. In RLHF, a
learned model of human preferences, <span
class="math inline">\(r_\theta(s_t, a_t)\)</span> (or any other
classification model) is used instead of an environmental reward
function. This gives the designer a substantial increase in the
flexibility of the approach and control over the final results.</li>
<li>No state transitions exist. In RLHF, the initial states for the
domain are prompts sampled from a training dataset and the “action” is
the completion to said prompt. During standard practices, this action
does not impact the next state and is only scored by the reward
model.</li>
<li>Response level rewards. Often referred to as a bandit problem, RLHF
attribution of reward is done for an entire sequence of actions,
composed of multiple generated tokens, rather than in a fine-grained
manner.</li>
</ol>
<p>Given the single-turn nature of the problem, the optimization can be
re-written without the time horizon and discount factor (and the reward
models): <span class="math display">\[J(\pi) = \mathbb{E}_{\tau \sim
\pi} \left[r_\theta(s_t, a_t) \right].\]</span> {#eq:rl_opt_int}</p>
<p>In many ways, the result is that while RLHF is heavily inspired by RL
optimizers and problem formulations, the action implementation is very
distinct from traditional RL.</p>
<figure id="fig:rlhf">
<img src="images/rlhf.png" alt="Standard RLHF loop" />
<figcaption aria-hidden="true">Standard RLHF loop</figcaption>
</figure>
<h2 data-number="1.3" id="optimization-tools"><span
class="header-section-number">1.3</span> Optimization Tools</h2>
<p>In this book, we detail many popular techniques for solving this
optimization problem. The popular tools of post-training include:</p>
<ul>
<li><strong>Reward modeling</strong> (Chapter 7): Where a model is
trained to capture the signal from collected preference data and can
then output a scalar reward indicating the quality of future text.</li>
<li><strong>Instruction finetuning</strong> (Chapter 9): A prerequisite
to RLHF where models are taught the question-answer format used in the
majority of language modeling interactions today by imitating
preselected examples.</li>
<li><strong>Rejection sampling</strong> (Chapter 10): The most basic
RLHF technique where candidate completions for instruction finetuning
are filtered by a reward model imitating human preferences.</li>
<li><strong>Policy gradients</strong> (Chapter 11): The reinforcement
learning algorithms used in the seminal examples of RLHF to update
parameters of a language model with respect to the signal from a reward
model.</li>
<li><strong>Direct alignment algorithms</strong> (Chapter 12):
Algorithms that directly optimize a policy from pairwise preference
data, rather than learning an intermediate reward model to then optimize
later.</li>
</ul>
<p>Modern RLHF-trained models always utilize instruction finetuning
followed by a mixture of the other optimization options.</p>
<h2 data-number="1.4" id="rlhf-recipe-example"><span
class="header-section-number">1.4</span> RLHF Recipe Example</h2>
<p>The canonical RLHF recipe circa the release of ChatGPT followed a
standard three step post-training recipe where RLHF was the center piece
<span class="citation"
data-cites="lambert2022illustrating">[@lambert2022illustrating]</span>
<span class="citation"
data-cites="ouyang2022training">[@ouyang2022training]</span> <span
class="citation" data-cites="bai2022training">[@bai2022training]</span>.
The three steps taken on top of a “base” language model (the next-token
prediction model trained on large-scale web text) was, summarized below
in <span class="citation"
data-cites="fig:rlhf-basic-repeat">@fig:rlhf-basic-repeat</span>:</p>
<ol type="1">
<li><strong>Instruction tuning on ~10K examples</strong>: This teaches
the model to follow the question-answer format and teaches some basic
skills from primarily human-written data.</li>
<li><strong>Training a reward model on ~100K pairwise prompts</strong>:
This model is trained from the instruction-tuned checkpoint and captures
the diverse values one wishes to model in their final training. The
reward model is the optimization target for RLHF.</li>
<li><strong>Training the instruction-tuned model with RLHF on another
~100K prompts</strong>: The model is optimized against the reward model
with a set of prompts that the model generates over before receiving
ratings.</li>
</ol>
<p>Once RLHF was done, the model was ready to be deployed to users. This
recipe is the foundation of modern RLHF, but recipes have evolved
substantially to include more stages and more data.</p>
<figure id="fig:rlhf-basic-repeat">
<img src="images/rlhf-basic.png"
alt="A rendition of the early, three stage RLHF process with SFT, a reward model, and then optimization." />
<figcaption aria-hidden="true">A rendition of the early, three stage
RLHF process with SFT, a reward model, and then
optimization.</figcaption>
</figure>
<p>Modern versions of post-training involve many, many more model
versions. An example is shown below in fig:complex-rlhf where the model
undergoes numerous training iterations before convergence.</p>
<figure id="fig:rlhf-complex">
<img src="images/rlhf-complex.png"
alt="A rendition of modern post-training with many rounds." />
<figcaption aria-hidden="true">A rendition of modern post-training with
many rounds.</figcaption>
</figure>
<h2 data-number="1.5" id="finetuning-and-regularization"><span
class="header-section-number">1.5</span> Finetuning and
Regularization</h2>
<p>RLHF is implemented from a strong base model, which induces a need to
control the optimization from straying too far from the initial policy.
In order to succeed in a finetuning regime, RLHF techniques employ
multiple types of regularization to control the optimization. The most
common change to the optimization function is to add a distance penalty
on the difference between the current RLHF policy and the starting point
of the optimization:</p>
<p><span class="math display">\[J(\pi) = \mathbb{E}_{\tau \sim \pi}
\left[r_\theta(s_t, a_t)\right] -
\beta  \mathcal{D}_{KL}(\pi^{\text{RL}}(\cdot|s_t) \|
\pi^{\text{ref}}(\cdot|s_t)).\]</span> {#eq:rlhf_opt_eq}</p>
<p>Within this formulation, a lot of study into RLHF training goes into
understanding how to spend a certain “KL budget” as measured by a
distance from the initial model. For more details, see Chapter 8 on
Regularization.</p>
</body>
</html>
