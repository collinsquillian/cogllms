<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>08-regularization</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/usr/share/javascript/mathjax/MathJax.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#regularization" id="toc-regularization"><span
class="toc-section-number">1</span> Regularization</a>
<ul>
<li><a href="#kl-distances-in-rl-optimization"
id="toc-kl-distances-in-rl-optimization"><span
class="toc-section-number">1.1</span> KL Distances in RL
Optimization</a>
<ul>
<li><a href="#reference-model-to-generations"
id="toc-reference-model-to-generations"><span
class="toc-section-number">1.1.1</span> Reference Model to
Generations</a></li>
<li><a href="#implementation-example"
id="toc-implementation-example"><span
class="toc-section-number">1.1.2</span> Implementation Example</a></li>
</ul></li>
<li><a href="#pretraining-gradients"
id="toc-pretraining-gradients"><span
class="toc-section-number">1.2</span> Pretraining Gradients</a></li>
<li><a href="#other-regularization" id="toc-other-regularization"><span
class="toc-section-number">1.3</span> Other Regularization</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="regularization"><span
class="header-section-number">1</span> Regularization</h1>
<p>Throughout the RLHF optimization, many regularization steps are used
to prevent over-optimization of the reward model. Over-optimization in
these contexts looks like models that output nonsensical text. Some
examples of optimization ``off the rails’’ are that models can output
followable math reasoning with extremely incorrect answers, repeated
text, switching languages, or excessive special characters.</p>
<p>The most popular variant, used in most RLHF implementations at the
time of writing, is a KL Distance from the current policy to a reference
policy across the generated samples. Many other regularization
techniques have emerged in the literature to then disappear in the next
model iteration in that line of research. That is to say that
regularization outside the core KL distance from generations is often
used to stabilize experimental setups that can then be simplified in the
next generations. Still, it is important to understand tools to
constrain optimization in RLHF.</p>
<p>The general formulation, when used in an RLHF framework with a reward
model, <span class="math inline">\(r_\theta\)</span> is as follows:</p>
<p><span class="math display">\[ r = r_\theta - \lambda r_{\text{reg.}}
\]</span> {#eq:rl_start}</p>
<p>With the reference implementation being:</p>
<p><span class="math display">\[
r = r_\theta - \lambda_{\text{KL}} \mathcal{D}_{\text{KL}} \left(
\pi^{\text{RL}}(y \mid x) \, \| \, \pi^{\text{Ref.}}(y \mid x) \right)
\]</span> {#eq:kl_standard}</p>
<h2 data-number="1.1" id="kl-distances-in-rl-optimization"><span
class="header-section-number">1.1</span> KL Distances in RL
Optimization</h2>
<p>For mathematical definitions, see Chapter 5 on Problem Setup. Recall
that KL distance is defined as follows:</p>
<p><span class="math display">\[ D_{KL}(P || Q) = \sum_{x \in
\mathcal{X}} P(x) \log \left(\frac{P(x)}{Q(x)}\right) \]</span></p>
<p>In RLHF, the two distributions of interest are often the distribution
of the new model version, say <span class="math inline">\(P(x)\)</span>,
and a distribution of the reference policy, say <span
class="math inline">\(Q(x)\)</span>.</p>
<h3 data-number="1.1.1" id="reference-model-to-generations"><span
class="header-section-number">1.1.1</span> Reference Model to
Generations</h3>
<p>The most common implementation of KL penalities are by comparing the
distance between the generated tokens during training to a static
reference model. The intuition is that the model you’re training from
has a style that you would like to stay close to. This reference model
is most often the instruction tuned model, but can also be a previous RL
checkpoint. With simple substitution, the model we are sampling from
becomes <span class="math inline">\(P^{\text{RL}}(x)\)</span> and <span
class="math inline">\(P^{\text{Ref.}}(x)\)</span>, shown above in <span
class="citation" data-cites="eq:kl_standard">@eq:kl_standard</span>.
Such KL distance was first applied to dialogue agents well before the
popularity of large language models <span class="citation"
data-cites="jaques2017sequence">[@jaques2017sequence]</span>, yet KL
control was quickly established as a core technique for fine-tuning
pretrained models <span class="citation"
data-cites="jaques2020human">[@jaques2020human]</span>.</p>
<h3 data-number="1.1.2" id="implementation-example"><span
class="header-section-number">1.1.2</span> Implementation Example</h3>
<p>In practice, the implementation of KL distance is often approximated
<span class="citation"
data-cites="schulman2016klapprox">[@schulman2016klapprox]</span>, making
the implementation far simpler. With the above definition, the summation
of KL can be converted to an expectation when sampling directly from the
distribution <span class="math inline">\(P(X)\)</span>. In this case,
the distribution <span class="math inline">\(P(X)\)</span> is the
generative distribution of the model currently being trained (i.e. not
the reference model). Then, the computation for KL distance changes to
the following:</p>
<p><span class="math display">\[
D_{\text{KL}}(P \,||\, Q) = \mathbb{E}_{x \sim P} \left[ \log P(x) -
\log Q(x) \right].
\]</span></p>
<p>This mode is far simpler to implement, particularly when dealing
directly with log probabilities used frequently in language model
training.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Generate tokens using the trained model&#39;s policy</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>generated_tokens <span class="op">=</span> model.generate(inputs)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Get logits for both models using the generated tokens as context</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model.forward(inputs) <span class="co"># technically redundant</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>ref_logits <span class="op">=</span> ref_model.forward(inputs)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>logprobs <span class="op">=</span> convert_to_logpbs(logits) <span class="co"># softmax and normalize</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>ref_logprobs <span class="op">=</span> convert_to_logpbs(ref_logits)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>kl_approx <span class="op">=</span> logprob <span class="op">-</span> ref_logprob</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>kl_full <span class="op">=</span> F.kl_div(ref_logprob, logprob) <span class="co"># alternate computation</span></span></code></pre></div>
<p>Some example implementations include <a
href="https://github.com/huggingface/trl/blob/5c21de30ae210e4251ead85517ba8dfe3f210e81/trl/trainer/ppo_trainer.py#L1150">TRL</a>
and <a
href="https://github.com/hamishivi/EasyLM/blob/main/EasyLM/models/llama/llama_train_ppo.py#L278">Hamish
Ivison’s Jax Code</a></p>
<h2 data-number="1.2" id="pretraining-gradients"><span
class="header-section-number">1.2</span> Pretraining Gradients</h2>
<p>Another way of viewing regularization is that you may have a
<em>dataset</em> that you want the model to remain close to, as done in
InstructGPT <span class="citation"
data-cites="ouyang2022training">[@ouyang2022training]</span> ‘’in order
to fix the performance regressions on public NLP datasets’’. To
implement this, they modify the training objective for RLHF. Taking
<span class="citation" data-cites="eq:rl_start">@eq:rl_start</span>, we
can transform this into an objective function to optimize by sampling
from the RL policy model, completions <span
class="math inline">\(y\)</span> from prompts <span
class="math inline">\(x\)</span>, which yields: <span
class="math display">\[
\text{objective} (\theta) = \mathbb{E}_{(x,y) \sim
\mathcal{D}_{\pi^{\text{RL}}_{\theta}}} \left[ r_{\theta}(x, y) -
\lambda r_{\text{reg.}} \right]
\]</span> Then, we can add an additional reward for higher probabilities
on pretraining accuracy: <span class="math display">\[
\text{objective} (\theta) = \mathbb{E}_{(x,y) \sim
\mathcal{D}_{\pi^{\text{RL}}_{\theta}}} \left[ r_{\theta}(x, y) -
\lambda r_{\text{reg.}} \right] + \gamma \mathbb{E}_{x \sim
\mathcal{D}_{\text{pretrain}}} \left[ \log(\pi^{\text{RL}}_{\theta}(x))
\right]
\]</span></p>
<p>Recent work proposed using a negative log likelihood term to balance
the optimization of Direct Preference Optimization (DPO) <span
class="citation"
data-cites="pang2024iterative">[@pang2024iterative]</span>. Given the
pairwise nature of the DPO loss, the same loss modification can be made
to reward model training, constraining the model to predict accurate
text (rumors from laboratories that did not publish the work).</p>
<p>The optimization follows as a modification to DPO. <span
class="math display">\[\mathcal{L}_{\text{DPO+NLL}} =
\mathcal{L}_{\text{DPO}}(c_i^w, y_i^w, c_i^l, y_i^l \mid x_i) + \alpha
\mathcal{L}_{\text{NLL}}(c_i^w, y_i^w \mid x_i)
\]</span></p>
<p><span class="math display">\[
= -\log \sigma \left( \beta \log \frac{M_\theta(c_i^w, y_i^w \mid
x_i)}{M_t(c_i^w, y_i^w \mid x_i)} - \beta \log \frac{M_\theta(c_i^l,
y_i^l \mid x_i)}{M_t(c_i^l, y_i^l \mid x_i)} \right) - \alpha \frac{\log
M_\theta(c_i^w, y_i^w \mid x_i)}{|c_i^w| + |y_i^w|}.
\]</span></p>
<p>TODO: Make the above equations congruent with the rest of the
notation on DPO.</p>
<h2 data-number="1.3" id="other-regularization"><span
class="header-section-number">1.3</span> Other Regularization</h2>
<p>Controlling the optimization is less well defined in other parts of
the RLHF stack. Most reward models have no regularization beyond the
standard contrastive loss function. Direct Alignment Algorithms handle
regularization to KL distances differently, through the <span
class="math inline">\(\beta\)</span> parameter (see the chapter on
Direct Alignment).</p>
<p>Llama 2 proposed a margin loss for reward model training <span
class="citation"
data-cites="touvron2023llama">[@touvron2023llama]</span>:</p>
<p><span class="math display">\[
\mathcal{L}(\theta) = - \left[ \log \left( \sigma \left( r_{\theta}(x,
y_w) - r_{\theta}(x, y_l) \right) - m(r) \right) \right]
\]</span></p>
<p>Where <span class="math inline">\(m(r)\)</span> is the numerical
difference in delta between the ratings of two annotators. This is
either achieved by having annotators rate the outputs on a numerical
scale or by using a quantified ranking method, such as <a
href="https://en.wikipedia.org/wiki/Likert_scale">Likert scales</a>.</p>
<p>Reward margins have been used heavily in the direct alignment
literature, such as Reward weighted DPO, ‘’Reward-aware Preference
Optimization’’ (RPO), which integrates reward model scores into the
update rule following a DPO loss <span class="citation"
data-cites="adler2024nemotron">[@adler2024nemotron]</span>, or REBEL
<span class="citation" data-cites="gao2024rebel">[@gao2024rebel]</span>
that has a reward delta weighting in a regression-loss formulation.</p>
</body>
</html>
