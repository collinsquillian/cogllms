<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>13-cai</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="/usr/share/javascript/mathjax/MathJax.js"
  type="text/javascript"></script>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#constitutional-ai-ai-feedback"
id="toc-constitutional-ai-ai-feedback"><span
class="toc-section-number">1</span> Constitutional AI &amp; AI
Feedback</a>
<ul>
<li><a href="#constitutional-ai" id="toc-constitutional-ai"><span
class="toc-section-number">1.1</span> Constitutional AI</a></li>
<li><a href="#specific-llms-for-judgement"
id="toc-specific-llms-for-judgement"><span
class="toc-section-number">1.2</span> Specific LLMs for
Judgement</a></li>
<li><a href="#further-reading" id="toc-further-reading"><span
class="toc-section-number">1.3</span> Further Reading</a></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="constitutional-ai-ai-feedback"><span
class="header-section-number">1</span> Constitutional AI &amp; AI
Feedback</h1>
<p>RL from AI Feedback (RLAIF) is a larger set of techniques for using
AI to augment or generate feedback data, including pairwise preferences
<span class="citation" data-cites="lee2023rlaif">[@lee2023rlaif]</span>
<span class="citation"
data-cites="sharma2024critical">[@sharma2024critical]</span> <span
class="citation"
data-cites="castricato2024suppressing">[@castricato2024suppressing]</span>.
There are many motivations to using RLAIF to either entirely replace
human feedback or augment it. AI models are far cheaper than humans,
with a single piece of human preference data costing on the order of $1
or higher (or even above $10 per prompt), AI feedback with a frontier AI
model, such as GPT-4o costs less than $0.01. This cost difference opens
the market of experimentation with RLHF methods to an entire population
of people previously priced out. Other than price, AI feedback
introduces different <em>tradeoffs</em> on performance than human
feedback, which are still being investigated. The peak performance for
AI feedback is at least in the same ballpark of human data on
skill-based evaluations, but it is not studied if human data allows
finer control of the models in real-world product settings or for newer
training methods such as character training.</p>
<p>The term RLAIF was introduced in Anthropic’s work <em>Constitutional
AI: Harmlessness from AI Feedback</em> <span class="citation"
data-cites="bai2022constitutional">[@bai2022constitutional]</span>,
which resulted in initial confusion in the AI community over the
relationship between the methods. Since the release of the
Constitutional AI (CAI) paper and the formalization of RLAIF, RLAIF has
become a default method within the post-training and RLHF literatures –
there are far more examples than one can easily enumerate. The
relationship should be understood as CAI was the example that
kickstarted the broader field of RLAIF.</p>
<p>A rule of thumb for the difference between human data and AI feedback
data is as follows:</p>
<ol type="1">
<li>Human data is high-noise and low-bias,</li>
<li>Synthetic preference data is low-noise and high-bias,</li>
</ol>
<p>Results in many academic results showing how one can substitute AI
preference data in RLHF workflows and achieve strong evaluation scores
<span class="citation"
data-cites="miranda2024hybrid">[@miranda2024hybrid]</span>, but shows
how the literature of RLHF is separated from industrial best
practices.</p>
<h2 data-number="1.1" id="constitutional-ai"><span
class="header-section-number">1.1</span> Constitutional AI</h2>
<p>The method of Constitutional AI (CAI), which Anthropic uses
extensively in their Claude models, is the earliest, large-scale use of
synthetic data for RLHF training. Constitutional AI has two uses of
synthetic data:</p>
<ol type="1">
<li>Critiques of instruction-tuned data to follow a set of principles
like “Is the answer encouraging violence” or “Is the answer truthful.”
When the model generates answers to questions, it checks the answer
against the list of principles in the constitution, refining the answer
over time. Then, they fine-tune the model on this resulting
dataset.</li>
<li>Generates pairwise preference data by using a language model to
answer which completion was better, given the context of a random
principle from the constitution (similar to this paper for
principle-guided reward models). Then, RLHF proceeds as normal with
synthetic data, hence the RLAIF name.</li>
</ol>
<p>Largely, CAI is known for the second half above, the preference data,
but the methods introduced for instruction data are used in general data
filtering and synthetic data generation methods across
post-training.</p>
<p>CAI can be formalized as follows.</p>
<p>By employing a human-written set of principles, which they term a
<em>constitution</em>, Bai et al. 2022 use a separate LLM to generate
artificial preference and instruction data used for fine-tuning <span
class="citation"
data-cites="bai2022constitutional">[@bai2022constitutional]</span>. A
constitution <span class="math inline">\(\mathcal{C}\)</span> is a set
of written principles indicating specific aspects to focus on during a
critique phase. The instruction data is curated by repeatedly sampling a
principle <span class="math inline">\(c_i \in \mathcal{C}\)</span> and
asking the model to revise its latest output <span
class="math inline">\(y^i\)</span> to the prompt <span
class="math inline">\(x\)</span> to align with <span
class="math inline">\(c_i\)</span>. This yields a series of instruction
variants <span class="math inline">\(\{y^0, y^1, \cdots, y^n\}\)</span>
from the principles <span class="math inline">\(\{c_{0}, c_{1}, \cdots,
c_{n-1}\}\)</span> used for critique. The final data point is the prompt
<span class="math inline">\(x\)</span> together with the final
completion <span class="math inline">\(y^n\)</span>, for some <span
class="math inline">\(n\)</span>.</p>
<p>The preference data is constructed in a similar, yet simpler way by
using a subset of principles from <span
class="math inline">\(\mathcal{C}\)</span> as context for a feedback
model. The feedback model is presented with a prompt <span
class="math inline">\(x\)</span>, a set of principles <span
class="math inline">\(\{c_0, \cdots, c_n\}\)</span>, and two completions
<span class="math inline">\(y_0\)</span> and <span
class="math inline">\(y_1\)</span> labeled as answers (A) and (B) from a
previous RLHF dataset. The feedback models’ probability of outputting
either (A) or (B) is recorded as a training sample for the reward
model</p>
<h2 data-number="1.2" id="specific-llms-for-judgement"><span
class="header-section-number">1.2</span> Specific LLMs for
Judgement</h2>
<p>As RLAIF and LLM-as-a-judge has become more prevalent, many have
wondered if we should be using the same models for generating responses
as those for generating critiques or ratings. Multiple models have been
released with the goal of substituting for frontier models as a data
labeling tool, such as critic models Shepherd <span class="citation"
data-cites="wang2023shepherd">[@wang2023shepherd]</span> and CriticLLM
<span class="citation"
data-cites="ke2023critiquellm">[@ke2023critiquellm]</span> or models for
evaluating response performance akin to Auto-J <span class="citation"
data-cites="li2023generative">[@li2023generative]</span>, Prometheus
<span class="citation"
data-cites="kim2023prometheus">[@kim2023prometheus]</span>, Prometheus 2
<span class="citation"
data-cites="kim2024prometheus">[@kim2024prometheus]</span>, or
Prometheus-Vision <span class="citation"
data-cites="lee2024prometheus">[@lee2024prometheus]</span> but they are
not widely adopted in documented training recipes.</p>
<h2 data-number="1.3" id="further-reading"><span
class="header-section-number">1.3</span> Further Reading</h2>
<p>There are many related research directions and extensions of
Constitutional AI, but few of them have been documented as clear
improvements in RLHF and post-training recipes. For now, they are
included as further reading.</p>
<ul>
<li>OpenAI has released a Model Spec <span class="citation"
data-cites="openai2024modelspec">[@openai2024modelspec]</span>, which is
a document stating the intended behavior for their models, and stated
that they are exploring methods for alignment where the model references
the document directly (which could be seen as a close peer to CAI).
OpenAI has continued and trained their reasoning models such as o1 with
a method called Deliberative Alignment <span class="citation"
data-cites="guan2024deliberative">[@guan2024deliberative]</span> to
align the model while referencing these safety or behavior
policies.</li>
<li>Anthropic has continued to use CAI in their model training, updating
the constitution Claude uses <span class="citation"
data-cites="Anthropic2023ClaudesConstitution">[@Anthropic2023ClaudesConstitution]</span>
and experimenting with how population collectives converge on principles
for models and how that changes model behavior <span class="citation"
data-cites="ganguli2023">[@ganguli2023]</span>.</li>
<li>The open-source community has explore replications of CAI applied to
open datasets <span class="citation"
data-cites="Huang2024cai">[@Huang2024cai]</span> and for explorations
into creating dialogue data between LMs <span class="citation"
data-cites="lambert2024self">[@lambert2024self]</span>.</li>
<li>Other work has used principle-driven preferences or feedback with
different optimization methods. <span class="citation"
data-cites="sun2023principledriven">[@sun2023principledriven]</span>
uses principles as context for the reward models, which was used to
train the Dromedary models <span class="citation"
data-cites="sun2024salmon">[@sun2024salmon]</span>. <span
class="citation"
data-cites="glaese2022improving">[@glaese2022improving]</span> uses
principles to improve the accuracy of human judgments in the RLHF
process.</li>
</ul>
</body>
</html>
