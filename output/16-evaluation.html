<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>16-evaluation</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#incomplete-evaluation-prompting"
id="toc-incomplete-evaluation-prompting"><span
class="toc-section-number">1</span> [Incomplete] Evaluation &amp;
Prompting</a>
<ul>
<li><a href="#how-to-tell-if-rlhf-is-working"
id="toc-how-to-tell-if-rlhf-is-working"><span
class="toc-section-number">1.1</span> How To Tell if RLHF is
Working?</a></li>
<li><a href="#prompting" id="toc-prompting"><span
class="toc-section-number">1.2</span> Prompting</a></li>
<li><a href="#evaluation" id="toc-evaluation"><span
class="toc-section-number">1.3</span> Evaluation</a>
<ul>
<li><a href="#formatting-and-overview"
id="toc-formatting-and-overview"><span
class="toc-section-number">1.3.1</span> Formatting and Overview</a></li>
<li><a href="#chatbotarena" id="toc-chatbotarena"><span
class="toc-section-number">1.3.2</span> ChatBotArena</a></li>
<li><a href="#private-leaderboards" id="toc-private-leaderboards"><span
class="toc-section-number">1.3.3</span> Private Leaderboards</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 data-number="1" id="incomplete-evaluation-prompting"><span
class="header-section-number">1</span> [Incomplete] Evaluation &amp;
Prompting</h1>
<h2 data-number="1.1" id="how-to-tell-if-rlhf-is-working"><span
class="header-section-number">1.1</span> How To Tell if RLHF is
Working?</h2>
<h2 data-number="1.2" id="prompting"><span
class="header-section-number">1.2</span> Prompting</h2>
<p>Prompting, i.e. crafting the correct query for a model, is a crucial
portion of using them as the models are evolving rapidly.</p>
<h2 data-number="1.3" id="evaluation"><span
class="header-section-number">1.3</span> Evaluation</h2>
<h3 data-number="1.3.1" id="formatting-and-overview"><span
class="header-section-number">1.3.1</span> Formatting and Overview</h3>
<h3 data-number="1.3.2" id="chatbotarena"><span
class="header-section-number">1.3.2</span> ChatBotArena</h3>
<p>ChatBotArena is the largest community evaluation tool for language
models. The LMSYS team, which emerged early in the post-ChatGPT craze,
works with most of the model providers to host all of the relevant
models. If you’re looking to get to know how multiple models compare to
each other, ChatBotArena is the place to start.</p>
<p>ChatBotArena casts language model evaluation through the wisdom of
the crowd. For getting an initial ranking of how models stack up and how
the models in the ecosystem are getting better, it has been and will
remain crucial.</p>
<p>ChatBotArena does not represent a controlled nor interpretable
experiment on language models.</p>
<p>When evaluating models to learn which are the best at extremely
challenging tasks, distribution control, and careful feedback are
necessary. For these reasons, ChatBotArena cannot definitively tell us
which models are solving the hardest tasks facing language models. It
does not measure how the best models are improving in clear ways. This
type of transparency comes elsewhere.</p>
<p>For most of its existence, people correlated the general capabilities
tested in ChatBotArena with a definitive ranking of which models can do
the hardest things for me. This is not true. In both my personal
experience reading data and what the community knows about the best
models, the ChatBotArena ranking shows the strongest correlations
with:</p>
<p>Certain stylistic outputs from language models, and</p>
<p>Language models that have high rates of complying with user
requests.</p>
<p>Both of these have been open research problems in the last two years.
Style is deeply intertwined with how information is received by the user
and precisely refusing only the most harmful requests is a deeply
challenging technical problem that both Meta (with Llama 2) and
Anthropic (with earlier versions of Claude particularly) have gotten
deeply criticized for.</p>
<p>Among closed labs, their styles have been greatly refined. All of
Meta, OpenAI, and Anthropic have distinctive styles (admittedly, I
haven’t used Google’s Gemini enough to know).</p>
<p>Meta’s AI is succinct and upbeat (something that has been discussed
many times on the LocalLlama subreddit).</p>
<p>OpenAI’s style is the most robotic to me. It answers as an AI and
contains a lot of information.</p>
<p>Claude’s style is intellectual, bordering on curious, and sometimes
quick to refuse.</p>
<p>When ChatBotArena was founded, these styles were in flux. Now, they
majorly shift the rankings depending on what people like. People seem to
like what OpenAI and Meta put out.</p>
<p>There are clear reasons why OpenAI’s models top the charts on
ChatBotArena. They were the originators of modern RLHF, have most
clearly dictated their goals with RLHF, continue to publish innovative
ideas in the space, and have always been ahead here. Most people just
did not realize how important this was to evaluation until the launch of
GPT-4o-mini. Culture impacts AI style.</p>
<h3 data-number="1.3.3" id="private-leaderboards"><span
class="header-section-number">1.3.3</span> Private Leaderboards</h3>
<p>Scale Leaderboard etc</p>
</body>
</html>
