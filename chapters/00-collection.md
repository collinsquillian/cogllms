# List of open leads

- The World's RL Gym: A history of LLM scaling
https://www.symbolic.capital/writing/the-worlds-rl-gym

    This chapter is supposed to present an overview and provide the intuition around how LLMs used to
    trained. This will naturally lead to the organic evolution of test-time compute and RL strategies being
    applied to LLMs today.
    20/23 Pretraining:
    - 2020: Scaling Laws for Neural Language Models by OpenAI
    - 2022: Training Compute-Optimal Large Language Models by DeepMind

    (Finetuning with either SFT or SFT + RHLF)

    24 Inference-time compute:
    - 2024: Chain of thought reasoning (graph of OpenAI's model performance on ARC exponential)
    - Outcompeting even larger models with inference-time compute: Scaling LLM Test-Time Compute Optimally
    can be More Effective than Scaling Model Parameters by DeepMind


    Use of RL with little human intervention for pre-training / finetuning and how it relates to inference
    time compute
    - RL frames: GRPO / DAPO / PPO

    - Scaling Laws for Neural Language Models
    https://arxiv.org/pdf/2001.08361
    - Training Compute-Optimal Large Language Models
    https://arxiv.org/pdf/2203.15556


- LLM Post-Training: A Deep Dive into Reasoning Large Language Models
https://arxiv.org/pdf/2502.21321



# RL with (verifiable) rewards

- TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training
Library

- Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains
https://arxiv.org/pdf/2503.23829